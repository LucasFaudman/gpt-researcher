"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3089],{8453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>t});var i=r(6540);const o={},s=i.createContext(o);function l(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),i.createElement(s.Provider,{value:n},e.children)}},9651:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>t,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"gpt-researcher/llms/llms","title":"Configure LLM","description":"As described in the introduction, the default LLM and embedding is OpenAI due to its superior performance and speed.","source":"@site/docs/gpt-researcher/llms/llms.md","sourceDirName":"gpt-researcher/llms","slug":"/gpt-researcher/llms/","permalink":"/docs/gpt-researcher/llms/","draft":false,"unlisted":false,"editUrl":"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/llms/llms.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Langsmith Logs","permalink":"/docs/gpt-researcher/handling-logs/langsmith-logs"},"next":{"title":"Supported LLMs","permalink":"/docs/gpt-researcher/llms/supported-llms"}}');var o=r(4848),s=r(8453);const l={},t="Configure LLM",a={},c=[{value:"OpenAI",id:"openai",level:2},{value:"Custom LLM",id:"custom-llm",level:2},{value:"Azure OpenAI",id:"azure-openai",level:2},{value:"Ollama",id:"ollama",level:2},{value:"Granite with Ollama",id:"granite-with-ollama",level:3},{value:"Groq",id:"groq",level:2},{value:"Sign up",id:"sign-up",level:3},{value:"Update env vars",id:"update-env-vars",level:3},{value:"Anthropic",id:"anthropic",level:2},{value:"Mistral AI",id:"mistral-ai",level:2},{value:"Together AI",id:"together-ai",level:2},{value:"HuggingFace",id:"huggingface",level:2},{value:"Google Gemini",id:"google-gemini",level:2},{value:"Google VertexAI",id:"google-vertexai",level:2},{value:"Cohere",id:"cohere",level:2},{value:"Fireworks",id:"fireworks",level:2},{value:"Bedrock",id:"bedrock",level:2},{value:"LiteLLM",id:"litellm",level:2},{value:"xAI",id:"xai",level:2},{value:"DeepSeek",id:"deepseek",level:2},{value:"Openrouter.ai",id:"openrouterai",level:2},{value:"Other Embedding Models",id:"other-embedding-models",level:2},{value:"Nomic",id:"nomic",level:3},{value:"VoyageAI",id:"voyageai",level:3}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"configure-llm",children:"Configure LLM"})}),"\n",(0,o.jsxs)(n.p,{children:["As described in the ",(0,o.jsx)(n.a,{href:"/docs/gpt-researcher/gptr/config",children:"introduction"}),", the default LLM and embedding is OpenAI due to its superior performance and speed.\nWith that said, GPT Researcher supports various open/closed source LLMs and embeddings, and you can easily switch between them by updating the ",(0,o.jsx)(n.code,{children:"SMART_LLM"}),", ",(0,o.jsx)(n.code,{children:"FAST_LLM"})," and ",(0,o.jsx)(n.code,{children:"EMBEDDING"})," env variables. You might also need to include the provider API key and corresponding configuration params."]}),"\n",(0,o.jsxs)(n.p,{children:["Current supported LLMs are ",(0,o.jsx)(n.code,{children:"openai"}),", ",(0,o.jsx)(n.code,{children:"anthropic"}),", ",(0,o.jsx)(n.code,{children:"azure_openai"}),", ",(0,o.jsx)(n.code,{children:"cohere"}),", ",(0,o.jsx)(n.code,{children:"google_vertexai"}),", ",(0,o.jsx)(n.code,{children:"google_genai"}),", ",(0,o.jsx)(n.code,{children:"fireworks"}),", ",(0,o.jsx)(n.code,{children:"ollama"}),", ",(0,o.jsx)(n.code,{children:"together"}),", ",(0,o.jsx)(n.code,{children:"mistralai"}),", ",(0,o.jsx)(n.code,{children:"huggingface"}),", ",(0,o.jsx)(n.code,{children:"groq"}),", ",(0,o.jsx)(n.code,{children:"bedrock"})," and ",(0,o.jsx)(n.code,{children:"litellm"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["Current supported embeddings are ",(0,o.jsx)(n.code,{children:"openai"}),", ",(0,o.jsx)(n.code,{children:"azure_openai"}),", ",(0,o.jsx)(n.code,{children:"cohere"}),", ",(0,o.jsx)(n.code,{children:"google_vertexai"}),", ",(0,o.jsx)(n.code,{children:"google_genai"}),", ",(0,o.jsx)(n.code,{children:"fireworks"}),", ",(0,o.jsx)(n.code,{children:"ollama"}),", ",(0,o.jsx)(n.code,{children:"together"}),", ",(0,o.jsx)(n.code,{children:"mistralai"}),", ",(0,o.jsx)(n.code,{children:"huggingface"}),", ",(0,o.jsx)(n.code,{children:"nomic"})," ,",(0,o.jsx)(n.code,{children:"voyageai"})," and ",(0,o.jsx)(n.code,{children:"bedrock"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["To learn more about support customization options see ",(0,o.jsx)(n.a,{href:"/docs/gpt-researcher/gptr/config",children:"here"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Please note"}),": GPT Researcher is optimized and heavily tested on GPT models. Some other models might run into context limit errors, and unexpected responses.\nPlease provide any feedback in our ",(0,o.jsx)(n.a,{href:"https://discord.gg/DUmbTebB",children:"Discord community"})," channel, so we can better improve the experience and performance."]}),"\n",(0,o.jsx)(n.p,{children:"Below you can find examples for how to configure the various supported LLMs."}),"\n",(0,o.jsx)(n.h2,{id:"openai",children:"OpenAI"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"# set the custom OpenAI API key\nOPENAI_API_KEY=[Your Key]\n\n# specify llms\nFAST_LLM=openai:gpt-4o-mini\nSMART_LLM=openai:gpt-4.1\nSTRATEGIC_LLM=openai:o4-mini\n\n# specify embedding\nEMBEDDING=openai:text-embedding-3-small\n"})}),"\n",(0,o.jsx)(n.h2,{id:"custom-llm",children:"Custom LLM"}),"\n",(0,o.jsxs)(n.p,{children:["Create a local OpenAI API using ",(0,o.jsx)(n.a,{href:"https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md#quick-start",children:"llama.cpp Server"}),"."]}),"\n",(0,o.jsx)(n.p,{children:'For custom LLM, specify "openai:{your-llm}"'}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"# set the custom OpenAI API url\nOPENAI_BASE_URL=http://localhost:1234/v1\n# set the custom OpenAI API key\nOPENAI_API_KEY=dummy_key\n\n# specify custom llms  \nFAST_LLM=openai:your_fast_llm\nSMART_LLM=openai:your_smart_llm\nSTRATEGIC_LLM=openai:your_strategic_llm\n"})}),"\n",(0,o.jsx)(n.p,{children:'For custom embedding, set "custom:{your-embedding}"'}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"# set the custom OpenAI API url\nOPENAI_BASE_URL=http://localhost:1234/v1\n# set the custom OpenAI API key\nOPENAI_API_KEY=dummy_key\n\n# specify the custom embedding model   \nEMBEDDING=custom:your_embedding\n"})}),"\n",(0,o.jsx)(n.h2,{id:"azure-openai",children:"Azure OpenAI"}),"\n",(0,o.jsxs)(n.p,{children:["In Azure OpenAI you have to chose which models you want to use and make deployments for each model. You do this on the ",(0,o.jsx)(n.a,{href:"https://portal.azure.com/",children:"Azure OpenAI Portal"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"In January 2025 the models that are recommended to use are:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"gpt-4o-mini"}),"\n",(0,o.jsx)(n.li,{children:"gpt-4o"}),"\n",(0,o.jsx)(n.li,{children:"o1-preview or o1-mini (You might need to request access to these models before you can deploy them)."}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["Please then specify the model names/deployment names in your ",(0,o.jsx)(n.code,{children:".env"})," file."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Required Precondition"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Your endpoint can have any valid name."}),"\n",(0,o.jsxs)(n.li,{children:["A model's deployment name ",(0,o.jsx)(n.em,{children:"must be the same"})," as the model name."]}),"\n",(0,o.jsxs)(n.li,{children:["You need to deploy an ",(0,o.jsx)(n.em,{children:"Embedding Model"}),": To ensure optimal performance, GPT Researcher requires the 'text-embedding-3-large' model. Please deploy this specific model to your Azure Endpoint."]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Recommended"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Quota increase: You should also request a quota increase especially for the embedding model, as the default quota is not sufficient."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:'# set the azure api key and deployment as you have configured it in Azure Portal. There is no default access point unless you configure it yourself!\nAZURE_OPENAI_API_KEY=[Your Key]\nAZURE_OPENAI_ENDPOINT=https://&#123;your-endpoint&#125;.openai.azure.com/\nOPENAI_API_VERSION=2024-05-01-preview\n\n# each string is "azure_openai:deployment_name". ensure that your deployment have the same name as the model you use!\nFAST_LLM=azure_openai:gpt-4o-mini\nSMART_LLM=azure_openai:gpt-4o\nSTRATEGIC_LLM=azure_openai:o1-preview\n\n# specify embedding\nEMBEDDING=azure_openai:text-embedding-3-large\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-azure-dynamic-sessions"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"ollama",children:"Ollama"}),"\n",(0,o.jsxs)(n.p,{children:["GPT Researcher supports both Ollama LLMs and embeddings. You can choose each or both.\nTo use ",(0,o.jsx)(n.a,{href:"http://www.ollama.com",children:"Ollama"})," you can set the following environment variables"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"OLLAMA_BASE_URL=http://localhost:11434\nFAST_LLM=ollama:llama3\nSMART_LLM=ollama:llama3\nSTRATEGIC_LLM=ollama:llama3\n\nEMBEDDING=ollama:nomic-embed-text\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-ollama"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h3,{id:"granite-with-ollama",children:"Granite with Ollama"}),"\n",(0,o.jsxs)(n.p,{children:["GPT Researcher has custom prompt formatting for the ",(0,o.jsx)(n.a,{href:"https://ollama.com/search?q=granite",children:"Granite family of models"}),". To use\nthe right formatting, you can set the following environment variables:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"OLLAMA_BASE_URL=http://localhost:11434\nFAST_LLM=ollama:granite3.3:2b\nSMART_LLM=ollama:granite3.3:8b\nSTRATEGIC_LLM=ollama:granite3.3:8b\nPROMPT_FAMILY=granite\n"})}),"\n",(0,o.jsx)(n.h2,{id:"groq",children:"Groq"}),"\n",(0,o.jsxs)(n.p,{children:["GroqCloud provides advanced AI hardware and software solutions designed to deliver amazingly fast AI inference performance.\nTo leverage Groq in GPT-Researcher, you will need a GroqCloud account and an API Key. (",(0,o.jsx)(n.strong,{children:"NOTE:"})," Groq has a very ",(0,o.jsx)(n.em,{children:"generous free tier"}),".)"]}),"\n",(0,o.jsx)(n.h3,{id:"sign-up",children:"Sign up"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["You can signup here: ",(0,o.jsx)(n.a,{href:"https://console.groq.com/login",children:"https://console.groq.com/login"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Once you are logged in, you can get an API Key here: ",(0,o.jsx)(n.a,{href:"https://console.groq.com/keys",children:"https://console.groq.com/keys"})]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Once you have an API key, you will need to add it to your ",(0,o.jsx)(n.code,{children:"systems environment"})," using the variable name:\n",(0,o.jsx)(n.code,{children:"GROQ_API_KEY=*********************"})]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"update-env-vars",children:"Update env vars"}),"\n",(0,o.jsx)(n.p,{children:"And finally, you will need to configure the GPT-Researcher Provider and Model variables:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"GROQ_API_KEY=[Your Key]\n\n# Set one of the LLM models supported by Groq\nFAST_LLM=groq:Mixtral-8x7b-32768\nSMART_LLM=groq:Mixtral-8x7b-32768\nSTRATEGIC_LLM=groq:Mixtral-8x7b-32768\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-groq"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"NOTE:"})," As of the writing of this Doc (May 2024), the available Language Models from Groq are:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Llama3-70b-8192"}),"\n",(0,o.jsx)(n.li,{children:"Llama3-8b-8192"}),"\n",(0,o.jsx)(n.li,{children:"Mixtral-8x7b-32768"}),"\n",(0,o.jsx)(n.li,{children:"Gemma-7b-it"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"anthropic",children:"Anthropic"}),"\n",(0,o.jsxs)(n.p,{children:["Refer to Anthropic ",(0,o.jsx)(n.a,{href:"https://docs.anthropic.com/en/api/getting-started",children:"Getting started page"})," to obtain Anthropic API key. Update the corresponding env vars, for example:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"ANTHROPIC_API_KEY=[Your Key]\nFAST_LLM=anthropic:claude-2.1\nSMART_LLM=anthropic:claude-3-opus-20240229\nSTRATEGIC_LLM=anthropic:claude-3-opus-20240229\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-anthropic"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.p,{children:"Anthropic does not offer its own embedding model, therefore, you'll want to either default to the OpenAI embedding model, or find another."}),"\n",(0,o.jsx)(n.h2,{id:"mistral-ai",children:"Mistral AI"}),"\n",(0,o.jsxs)(n.p,{children:["Sign up for a ",(0,o.jsx)(n.a,{href:"https://console.mistral.ai/users/api-keys/",children:"Mistral API key"}),".\nThen update the corresponding env vars, for example:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"MISTRAL_API_KEY=[Your Key]\nFAST_LLM=mistralai:open-mistral-7b\nSMART_LLM=mistralai:mistral-large-latest\nSTRATEGIC_LLM=mistralai:mistral-large-latest\n\nEMBEDDING=mistralai:mistral-embed\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-mistralai"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"together-ai",children:"Together AI"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://www.together.ai/",children:"Together AI"})," offers an API to query ",(0,o.jsx)(n.a,{href:"https://docs.together.ai/docs/inference-models",children:"50+ leading open-source models"})," in a couple lines of code.\nThen update corresponding env vars, for example:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"TOGETHER_API_KEY=[Your Key]\nFAST_LLM=together:meta-llama/Llama-3-8b-chat-hf\nSMART_LLM=together:meta-llama/Llama-3-70b-chat-hf\nSTRATEGIC_LLM=together:meta-llama/Llama-3-70b-chat-hf\n\nEMBEDDING=mistralai:nomic-ai/nomic-embed-text-v1.5\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-together"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"huggingface",children:"HuggingFace"}),"\n",(0,o.jsxs)(n.p,{children:["This integration requires a bit of extra work. Follow ",(0,o.jsx)(n.a,{href:"https://python.langchain.com/v0.1/docs/integrations/chat/huggingface/",children:"this guide"})," to learn more.\nAfter you've followed the tutorial above, update the env vars:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"HUGGINGFACE_API_KEY=[Your Key]\nFAST_LLM=huggingface:HuggingFaceH4/zephyr-7b-beta\nSMART_LLM=huggingface:HuggingFaceH4/zephyr-7b-beta\nSTRATEGIC_LLM=huggingface:HuggingFaceH4/zephyr-7b-beta\n\nEMBEDDING=huggingface:sentence-transformers/all-MiniLM-L6-v2\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-huggingface"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"google-gemini",children:"Google Gemini"}),"\n",(0,o.jsxs)(n.p,{children:["Sign up ",(0,o.jsx)(n.a,{href:"https://ai.google.dev/gemini-api/docs/api-key",children:"here"})," for obtaining a Google Gemini API Key and update the following env vars:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"GOOGLE_API_KEY=[Your Key]\nFAST_LLM=google_genai:gemini-1.5-flash\nSMART_LLM=google_genai:gemini-1.5-pro\nSTRATEGIC_LLM=google_genai:gemini-1.5-pro\n\nEMBEDDING=google_genai:models/text-embedding-004\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-google-genai"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"google-vertexai",children:"Google VertexAI"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"FAST_LLM=google_vertexai:gemini-1.5-flash-001\nSMART_LLM=google_vertexai:gemini-1.5-pro-001\nSTRATEGIC_LLM=google_vertexai:gemini-1.5-pro-001\n\nEMBEDDING=google_vertexai:text-embedding-004\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-google-vertexai"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"cohere",children:"Cohere"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"COHERE_API_KEY=[Your Key]\nFAST_LLM=cohere:command\nSMART_LLM=cohere:command-nightly\nSTRATEGIC_LLM=cohere:command-nightly\n\nEMBEDDING=cohere:embed-english-v3.0\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-cohere"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"fireworks",children:"Fireworks"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"FIREWORKS_API_KEY=[Your Key]\nbase_url=https://api.fireworks.ai/inference/v1/completions\nFAST_LLM=fireworks:accounts/fireworks/models/mixtral-8x7b-instruct\nSMART_LLM=fireworks:accounts/fireworks/models/mixtral-8x7b-instruct\nSTRATEGIC_LLM=fireworks:accounts/fireworks/models/mixtral-8x7b-instruct\n\nEMBEDDING=fireworks:nomic-ai/nomic-embed-text-v1.5\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-fireworks"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"bedrock",children:"Bedrock"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"FAST_LLM=bedrock:anthropic.claude-3-sonnet-20240229-v1:0\nSMART_LLM=bedrock:anthropic.claude-3-sonnet-20240229-v1:0\nSTRATEGIC_LLM=bedrock:anthropic.claude-3-sonnet-20240229-v1:0\n\nEMBEDDING=bedrock:amazon.titan-embed-text-v2:0\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain_aws"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"litellm",children:"LiteLLM"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"FAST_LLM=litellm:perplexity/pplx-7b-chat\nSMART_LLM=litellm:perplexity/pplx-70b-chat\nSTRATEGIC_LLM=litellm:perplexity/pplx-70b-chat\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain_community"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"xai",children:"xAI"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"FAST_LLM=xai:grok-beta\nSMART_LLM=xai:grok-beta\nSTRATEGIC_LLM=xai:grok-beta\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain_xai"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]}),"\n",(0,o.jsx)(n.h2,{id:"deepseek",children:"DeepSeek"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"DEEPSEEK_API_KEY=[Your Key]\nFAST_LLM=deepseek:deepseek-chat\nSMART_LLM=deepseek:deepseek-chat\nSTRATEGIC_LLM=deepseek:deepseek-chat\n"})}),"\n",(0,o.jsx)(n.h2,{id:"openrouterai",children:"Openrouter.ai"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"OPENROUTER_API_KEY=[Your openrouter.ai key]\nOPENAI_BASE_URL=https://openrouter.ai/api/v1\nFAST_LLM=openrouter:google/gemini-2.0-flash-lite-001\nSMART_LLM=openrouter:google/gemini-2.0-flash-001\nSTRATEGIC_LLM=openrouter:google/gemini-2.5-pro-exp-03-25\nOPENROUTER_LIMIT_RPS=1  # Ratelimit request per secound\nEMBEDDING=google_genai:models/text-embedding-004 # openrouter doesn't support embedding models, use google instead its free\nGOOGLE_API_KEY=[Your *google gemini* key]\n"})}),"\n",(0,o.jsx)(n.h2,{id:"other-embedding-models",children:"Other Embedding Models"}),"\n",(0,o.jsx)(n.h3,{id:"nomic",children:"Nomic"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"EMBEDDING=nomic:nomic-embed-text-v1.5\n"})}),"\n",(0,o.jsx)(n.h3,{id:"voyageai",children:"VoyageAI"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-env",children:"VOYAGE_API_KEY=[Your Key]\nEMBEDDING=voyageai:voyage-law-2\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Add ",(0,o.jsx)(n.code,{children:"langchain-voyageai"})," to ",(0,o.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt",children:"requirements.txt"})," for Docker Support or ",(0,o.jsx)(n.code,{children:"pip install"})," it"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);