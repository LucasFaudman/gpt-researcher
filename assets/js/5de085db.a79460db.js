"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1736],{2932:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>i,contentTitle:()=>c,default:()=>m,frontMatter:()=>l,metadata:()=>n,toc:()=>a});const n=JSON.parse('{"id":"gpt-researcher/llms/testing-your-llm","title":"Testing your LLM","description":"Here is a snippet of code to help you verify that your LLM-related environment variables are set up correctly.","source":"@site/docs/gpt-researcher/llms/testing-your-llm.md","sourceDirName":"gpt-researcher/llms","slug":"/gpt-researcher/llms/testing-your-llm","permalink":"/docs/gpt-researcher/llms/testing-your-llm","draft":false,"unlisted":false,"editUrl":"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/llms/testing-your-llm.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Supported LLMs","permalink":"/docs/gpt-researcher/llms/supported-llms"},"next":{"title":"Running with Azure","permalink":"/docs/gpt-researcher/llms/running-with-azure"}}');var s=t(4848),o=t(8453);const l={},c="Testing your LLM",i={},a=[];function p(e){const r={code:"code",h1:"h1",header:"header",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"testing-your-llm",children:"Testing your LLM"})}),"\n",(0,s.jsx)(r.p,{children:"Here is a snippet of code to help you verify that your LLM-related environment variables are set up correctly."}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from gpt_researcher.config.config import Config\nfrom gpt_researcher.utils.llm import create_chat_completion\nimport asyncio\nfrom dotenv import load_dotenv\nload_dotenv()\n\nasync def main():\n    cfg = Config()\n\n    try:\n        report = await create_chat_completion(\n            model=cfg.smart_llm_model,\n            messages = [{"role": "user", "content": "sup?"}],\n            temperature=0.35,\n            llm_provider=cfg.smart_llm_provider,\n            stream=True,\n            max_tokens=cfg.smart_token_limit,\n            llm_kwargs=cfg.llm_kwargs\n        )\n    except Exception as e:\n        print(f"Error in calling LLM: {e}")\n\n# Run the async function\nasyncio.run(main())\n'})})]})}function m(e={}){const{wrapper:r}={...(0,o.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453:(e,r,t)=>{t.d(r,{R:()=>l,x:()=>c});var n=t(6540);const s={},o=n.createContext(s);function l(e){const r=n.useContext(o);return n.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function c(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),n.createElement(o.Provider,{value:r},e.children)}}}]);