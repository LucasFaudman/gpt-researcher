"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5894],{6042:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"stepping-into-the-story","metadata":{"permalink":"/blog/stepping-into-the-story","source":"@site/blog/2025-03-10-stepping-into-the-story/index.md","title":"Stepping Into the Story of GPT Researcher","description":"GPTR reflecting ourselves","date":"2025-03-10T00:00:00.000Z","formattedDate":"March 10, 2025","tags":[{"label":"ai","permalink":"/blog/tags/ai"},{"label":"gpt-researcher","permalink":"/blog/tags/gpt-researcher"},{"label":"prompts","permalink":"/blog/tags/prompts"},{"label":"dreams","permalink":"/blog/tags/dreams"},{"label":"community","permalink":"/blog/tags/community"}],"readingTime":4.98,"truncated":false,"authors":[{"name":"Elisha Kramer","title":"Core Contributor @ GPT Researcher","url":"https://github.com/ElishaKay","imageURL":"https://avatars.githubusercontent.com/u/16700452","key":"elishakay"}],"nextItem":{"title":"Introducing Deep Research: The Open Source Alternative","permalink":"/blog/2025/02/26/deep-research"}},"content":"![GPTR reflecting ourselves](https://github.com/user-attachments/assets/f6e8a6b5-12f8-4faa-ae99-6a2fbaf23cc1)\\n\\n## The Barnes & Noble Dream\\n\\nAs a teenager, I remember stepping into Barnes & Noble, the scent of fresh pages filling the air, my fingers tracing the spines of books that had shaped minds and captured hearts. I\'d whisper to myself: One day, my name will be here.\\n\\nTo me, books weren\'t just stories\u2014they were reflections of the human experience, ways for people to see themselves more clearly. Shakespeare once said, \u201cThe purpose of art is to hold a mirror up to nature.\u201d That idea stuck with me. Art, writing, and storytelling weren\'t just about entertainment; they were about understanding ourselves in new ways.\\n\\nBut the world changed. The bookstores faded, attention shifted, and the novel\u2014once the pinnacle of deep thought and reflection\u2014gave way to new forms of engagement. The long, immersive experience of reading was replaced with something more dynamic, more interactive.\\n\\n## The Journey into Coding: A Simba Moment\\n\\nAbout 9 years ago, [much like Simba in The Lion King](https://open.spotify.com/track/3BUT32qmBXmlqp3EJkgRfp?si=0935ef6eedf247ed), I embarked on a new journey filled with doubt and uncertainty. Leaving my known world of writing, I stepped into the unknown realm of coding. It was a foreign language at first\u2014endless lines of syntax, debugging errors that made no sense, and moments of frustration where I felt like an imposter in a world of developers.\\n\\nThe journey was tough\u2014I struggled to find my place, faced canceled contracts, and got my butt handed to me more times than I could count. Every rejection, every missed opportunity made me question if I had taken the wrong path. Maybe I wasn\'t meant to build\u2014maybe I was meant to stay in the world of stories.\\n\\nEven when I finally landed a job at Fiverr, working with JavaScript, MySQL, HTML, and CSS, I still felt like I had abandoned my identity as a writer.\\n\\n## Discovering GPT Researcher\\n\\nOne night, about a year ago, deep into a rabbit hole of AI research, I stumbled upon GPT Researcher. The concept struck me instantly\u2014AI wasn\'t just a tool; it was a means of expanding human knowledge, refining our questions, and reshaping how we approach research itself.\\n\\nI reached out to Assaf, not expecting much. But instead of a polite acknowledgment, he welcomed me in. That moment\u2014seeing my first commit merged\u2014felt like an echo of my old dream. Only this time, I wasn\'t just writing stories. I was building something that helped others uncover their own.\\n\\n## The Wicked Witch of the Researcher\'s Mirror\\n\\nAround that time, I found myself repeatedly asking GPT Researcher the same question:\\n\\n\\"Who is Elisha Kramer?\\"\\n\\nAt first, it was like the Magic Mirror in Snow White, responding with something generic like, \\"Elisha Kramer is a software engineer with experience in web development.\\" It pulled information from my LinkedIn, GitHub, and Udemy profiles, painting a picture of who I was professionally. But then, things got weird.\\n\\nI made more commits to GPT Researcher. More contributions. And as I coded, I asked a different question.\\n\\n\\"Who is ElishaKay on Github?\\"\\n\\nAs time went on, the answer changed since the Researcher was pulling new sources fresh off web search results.\\n\\n\\"ElishaKay is an active open source contributor with multiple repositories and over 500 commits in the past year.\\"\\n\\nHoly Shnikes! It was learning. Another commit. Another feature. Another line of documentation. Time to get more specific.\\n\\n\\"Who is ElishaKay of gpt-researcher?\\"\\n\\n\\"ElishaKay is a core contributor of GPT Researcher, improving research workflows and enhancing AI retrieval through significant code and documentation contributions.\\"\\n\\nNow we were talking. But I wasn\'t done. Like the Wicked Witch, I kept coming back. More commits. More improvements. More features.\\n\\nUntil finally, I asked:\\n\\n\\"Tell me about gpt-researcher and tips to improve it\\"\\n\\nAnd GPT Researcher looked back at me and said:\\n\\n\\"GPTR is a thriving open-source community. The best path forward is to continue investing in that community - through code contributions, documentation improvements, and helping new contributors get started. The project\'s strength lies in its collaborative nature.\\"\\n\\nAnd that\'s when I knew\u2014I wasn\'t just using GPT Researcher. I was becoming part of its story.\\n\\n## AI as a mirror of ourselves\\n\\nThis evolving feedback helped me frame my own self-narrative. GPT Researcher wasn\'t just reflecting what was already known\u2014it was pulling in context from both my work and the broader internet.\\n\\nIt was reflecting back my own journey, refining it with each step, blurring the illusion of a fixed identity, and embracing an evolving one.\\n\\nEvery query, every commit, every improvement shaped the tool\u2014and in turn, it shaped me.\\n\\n## Building as a Community\\n\\nGPT Researcher isn\'t just a tool. It\'s a reflection of the open-source spirit, a living, evolving ecosystem where knowledge isn\'t static but constantly refined. It isn\'t just answering questions; it\'s engaging in a dialogue, shaping and reshaping narratives based on the latest contributions, research, and discoveries.\\nIt isn\'t just about me anymore. It\'s about us.\\nA network of 138 contributors. An open-source project watched by 20,000 stars. A collective movement pushing the boundaries of AI-driven research.\\n\\nEvery researcher, every developer, every curious mind who refines their questions, contributes a feature, or engages with the tool is part of something bigger. AI isn\'t just some black box spitting out answers\u2014it\'s a tool that helps us refine our own thinking, challenge assumptions, and expand our understanding.\\nIt\'s an iterative process, just like life itself.\\nThe more context we provide, the better the insights we get. The more we engage, the more it reflects back not just who we were but who we are becoming.\\n\\n## A Story Still Being Written\\n\\nSo while I once dreamed of seeing my name on a book spine in Barnes & Noble, I now see something even greater.\\nMy words aren\'t bound to a single book\u2014they live within every line of code, every contribution, every researcher refining their questions.\\nWe are not just users. We are builders.\\nAnd this isn\'t just my story.\\nIt\'s our story.\\nAnd it\'s still being written."},{"id":"Introducing Deep Research: The Open Source Alternative","metadata":{"permalink":"/blog/2025/02/26/deep-research","source":"@site/blog/2025-02-26-deep-research/index.md","title":"Introducing Deep Research: The Open Source Alternative","description":"The Dawn of Deep Research in AI","date":"2025-02-26T00:00:00.000Z","formattedDate":"February 26, 2025","tags":[],"readingTime":7.47,"truncated":false,"authors":[],"prevItem":{"title":"Stepping Into the Story of GPT Researcher","permalink":"/blog/stepping-into-the-story"},"nextItem":{"title":"The Future of Research is Hybrid","permalink":"/blog/gptr-hybrid"}},"content":"## The Dawn of Deep Research in AI\\n\\nThe AI research landscape is witnessing a revolutionary shift with the emergence of \\"Deep Research\\" capabilities. But what exactly is deep research, and why should you care? \\n\\nDeep research represents the next evolution in AI-powered information retrieval - going far beyond simple search to deliver comprehensive, multi-layered analysis of complex topics. Unlike traditional search engines that return a list of links, or even first-generation AI assistants that provide surface-level summaries, deep research tools deploy sophisticated algorithms to explore topics with unprecedented depth and breadth, mimicking the way human researchers would tackle complex subjects.\\n\\nThe key features that define true deep research capabilities include iterative analysis that refines queries and results dynamically ([InfoQ, 2025](https://www.infoq.com/news/2025/02/perplexity-deep-research/)), multimodal processing that integrates diverse data formats ([Observer, 2025](https://observer.com/2025/01/openai-google-gemini-agi/)), real-time data retrieval for up-to-date insights ([WinBuzzer, 2025](https://winbuzzer.com/2025/02/15/perplexity-deep-research-challenges-openai-and-googles-ai-powered-information-retrieval-xcxwbn/)), and structured outputs with proper citations for academic and technical applications ([Helicone, 2025](https://www.helicone.ai/blog/openai-deep-research)).\\n\\nIn recent months, we\'ve seen major players launch their own deep research solutions, each with its unique approach and positioning in the market:\\n\\n- **Perplexity AI** focuses on speed, delivering research results in under three minutes with real-time data retrieval ([Analytics Vidhya, 2025](https://www.analyticsvidhya.com/blog/2025/02/perplexity-deep-research/)). Their cost-effective model (starting at free tier) makes advanced research accessible to a broader audience, though some analysts note potential accuracy trade-offs in favor of speed ([Medium, 2025](https://medium.com/towards-agi/perplexity-ai-deep-research-vs-openai-deep-research-an-in-depth-comparison-6784c814fc4a)).\\n\\n- **OpenAI\'s Deep Research** (built on the O3 model) prioritizes depth and precision, excelling in technical and academic applications with advanced reasoning capabilities ([Helicone, 2025](https://www.helicone.ai/blog/openai-deep-research)). Their structured outputs include detailed citations, ensuring reliability and verifiability. However, at $200/month ([Opentools, 2025](https://opentools.ai/news/openai-unveils-groundbreaking-deep-research-chatgpt-for-pro-users)), it represents a significant investment, and comprehensive reports can take 5-30 minutes to generate ([ClickItTech, 2025](https://www.clickittech.com/ai/perplexity-deep-research-vs-openai-deep-research/)).\\n\\n- **Google\'s Gemini 2.0** emphasizes multimodal integration across text, images, audio, and video, with particular strength in enterprise applications ([Adyog, 2024](https://blog.adyog.com/2024/12/31/the-ai-titans-face-off-openais-o3-vs-googles-gemini-2-0/)). At $20/month, it offers a more affordable alternative to OpenAI\'s solution, though some users note limitations in customization flexibility ([Helicone, 2025](https://www.helicone.ai/blog/openai-deep-research)).\\n\\nWhat makes deep research truly exciting is its potential to democratize advanced knowledge synthesis ([Medium, 2025](https://medium.com/@greeshmamshajan/the-evolution-of-ai-powered-research-perplexitys-disruption-and-the-battle-for-cognitive-87af682cc8e6)), dramatically enhance productivity by automating time-intensive research tasks ([The Mobile Indian, 2025](https://www.themobileindian.com/news/perplexity-deep-research-vs-openai-deep-research-vs-gemini-1-5-pro-deep-research-ai-fight)), and open new avenues for interdisciplinary research through advanced reasoning capabilities ([Observer, 2025](https://observer.com/2025/01/openai-google-gemini-agi/)).\\n\\nHowever, a key limitation in the current market is accessibility - the most powerful deep research tools remain locked behind expensive paywalls or closed systems, putting them out of reach for many researchers, students, and smaller organizations who could benefit most from these capabilities.\\n\\n## Introducing GPT Researcher Deep Research \u2728\\n\\nWe\'re thrilled to announce our answer to this trend: **GPT Researcher Deep Research** - an advanced open-source recursive research system that explores topics with depth and breadth, all while maintaining cost-effectiveness and transparency.\\n\\n[GPT Researcher](https://github.com/assafelovic/gpt-researcher) Deep Research not only matches the capabilities of the industry giants but exceeds them in several key metrics:\\n\\n- **Cost-effective**: Each deep research operation costs approximately $0.40 (using `o3-mini` on `\\"high\\"` reasoning effort)\\n- **Time-efficient**: Complete research in around 5 minutes\\n- **Fully customizable**: Adjust parameters to match your specific research needs\\n- **Transparent**: Full visibility into the research process and methodology\\n- **Open source**: Free to use, modify, and integrate into your workflows\\n\\n## How It Works: The Recursive Research Tree\\n\\nWhat makes GPT Researcher\'s deep research so powerful is its tree-like exploration pattern that combines breadth and depth in an intelligent, recursive approach:\\n\\n![Research Flow Diagram](https://github.com/user-attachments/assets/eba2d94b-bef3-4f8d-bbc0-f15bd0a40968)\\n\\n1. **Breadth Exploration**: At each level, it generates multiple search queries to explore different aspects of your topic\\n2. **Depth Diving**: For each branch, it recursively goes deeper, following promising leads and uncovering hidden connections\\n3. **Concurrent Processing**: Utilizing async/await patterns to run multiple research paths simultaneously\\n4. **Context Management**: Automatically aggregates and synthesizes findings across all branches\\n5. **Real-time Tracking**: Provides updates on research progress across both breadth and depth dimensions\\n\\nImagine deploying a team of AI researchers, each following their own research path while collaborating to build a comprehensive understanding of your topic. That\'s the power of GPT Researcher\'s deep research approach.\\n\\n## Getting Started in Minutes\\n\\nIntegrating deep research into your projects is remarkably straightforward:\\n\\n```python\\nfrom gpt_researcher import GPTResearcher\\nimport asyncio\\n\\nasync def main():\\n    # Initialize researcher with deep research type\\n    researcher = GPTResearcher(\\n        query=\\"What are the latest developments in quantum computing?\\",\\n        report_type=\\"deep\\",  # This triggers deep research mode\\n    )\\n    \\n    # Run research\\n    research_data = await researcher.conduct_research()\\n    \\n    # Generate report\\n    report = await researcher.write_report()\\n    print(report)\\n\\nif __name__ == \\"__main__\\":\\n    asyncio.run(main())\\n```\\n\\n## Under the Hood: How Deep Research Works\\n\\nLooking at the codebase reveals the sophisticated system that powers GPT Researcher\'s deep research capabilities:\\n\\n### 1. Query Generation and Planning\\n\\nThe system begins by generating a set of diverse search queries based on your initial question:\\n\\n```python\\nasync def generate_search_queries(self, query: str, num_queries: int = 3) -> List[Dict[str, str]]:\\n    \\"\\"\\"Generate SERP queries for research\\"\\"\\"\\n    messages = [\\n        {\\"role\\": \\"system\\", \\"content\\": \\"You are an expert researcher generating search queries.\\"},\\n        {\\"role\\": \\"user\\",\\n         \\"content\\": f\\"Given the following prompt, generate {num_queries} unique search queries to research the topic thoroughly. For each query, provide a research goal. Format as \'Query: <query>\' followed by \'Goal: <goal>\' for each pair: {query}\\"}\\n    ]\\n```\\n\\nThis process creates targeted queries, each with a specific research goal. For example, a query about quantum computing might generate:\\n- \\"Latest quantum computing breakthroughs 2024-2025\\"\\n- \\"Quantum computing practical applications in finance\\"\\n- \\"Quantum error correction advancements\\"\\n\\n### 2. Concurrent Research Execution\\n\\nThe system then executes these queries concurrently, with intelligent resource management:\\n\\n```python\\n# Process queries with concurrency limit\\nsemaphore = asyncio.Semaphore(self.concurrency_limit)\\n\\nasync def process_query(serp_query: Dict[str, str]) -> Optional[Dict[str, Any]]:\\n    async with semaphore:\\n        # Research execution logic\\n```\\n\\nThis approach maximizes efficiency while ensuring system stability - like having multiple researchers working in parallel.\\n\\n### 3. Recursive Exploration\\n\\nThe magic happens with recursive exploration:\\n\\n```python\\n# Continue deeper if needed\\nif depth > 1:\\n    new_breadth = max(2, breadth // 2)\\n    new_depth = depth - 1\\n    progress.current_depth += 1\\n\\n    # Create next query from research goal and follow-up questions\\n    next_query = f\\"\\"\\"\\n    Previous research goal: {result[\'researchGoal\']}\\n    Follow-up questions: {\' \'.join(result[\'followUpQuestions\'])}\\n    \\"\\"\\"\\n\\n    # Recursive research\\n    deeper_results = await self.deep_research(\\n        query=next_query,\\n        breadth=new_breadth,\\n        depth=new_depth,\\n        # Additional parameters\\n    )\\n```\\n\\nThis creates a tree-like exploration pattern that follows promising leads deeper while maintaining breadth of coverage.\\n\\n### 4. Context Management and Synthesis\\n\\nManaging the vast amount of gathered information requires sophisticated tracking:\\n\\n```python\\n# Trim context to stay within word limits\\ntrimmed_context = trim_context_to_word_limit(all_context)\\nlogger.info(f\\"Trimmed context from {len(all_context)} items to {len(trimmed_context)} items to stay within word limit\\")\\n```\\n\\nThis ensures the most relevant information is retained while respecting model context limitations.\\n\\n## Customizing Your Research Experience\\n\\nOne of the key advantages of GPT Researcher\'s open-source approach is full customizability. You can tailor the research process to your specific needs through several configuration options:\\n\\n```yaml\\ndeep_research_breadth: 4    # Number of parallel research paths\\ndeep_research_depth: 2      # How many levels deep to explore\\ndeep_research_concurrency: 4  # Maximum concurrent operations\\ntotal_words: 2500           # Word count for final report\\n```\\n\\nApply these configurations through environment variables, a config file, or directly in code:\\n\\n```python\\nresearcher = GPTResearcher(\\n    query=\\"your query\\",\\n    report_type=\\"deep\\",\\n    config_path=\\"path/to/config.yaml\\"\\n)\\n```\\n\\n## Real-time Progress Tracking\\n\\nFor applications requiring visibility into the research process, GPT Researcher provides detailed progress tracking:\\n\\n```python\\nclass ResearchProgress:\\n    current_depth: int       # Current depth level\\n    total_depth: int         # Maximum depth to explore\\n    current_breadth: int     # Current number of parallel paths\\n    total_breadth: int       # Maximum breadth at each level\\n    current_query: str       # Currently processing query\\n    completed_queries: int   # Number of completed queries\\n    total_queries: int       # Total queries to process\\n```\\n\\nThis allows you to build interfaces that show research progress in real-time - perfect for applications where users want visibility into the process.\\n\\n## Why This Matters: The Impact of Deep Research\\n\\nThe democratization of deep research capabilities through open-source tools like GPT Researcher represents a paradigm shift in how we process and analyze information. Benefits include:\\n\\n1. **Deeper insights**: Uncover connections and patterns that surface-level research would miss\\n2. **Time savings**: Automate hours or days of manual research into minutes\\n3. **Reduced costs**: Enterprise-grade research capabilities at a fraction of the cost\\n4. **Accessibility**: Bringing advanced research tools to individuals and small organizations\\n5. **Transparency**: Full visibility into the research methodology and sources\\n\\n## Getting Started Today\\n\\nReady to experience the power of deep research in your projects? Here\'s how to get started:\\n\\n1. **Installation**: `pip install gpt-researcher`\\n2. **API Key**: Set up your API key for the LLM provider and search engine of your choice\\n3. **Configuration**: Customize parameters based on your research needs\\n4. **Implementation**: Use the example code to integrate into your application\\n\\nMore detailed instructions and examples can be found in the [GPT Researcher documentation](https://docs.gptr.dev/docs/gpt-researcher/gptr/deep_research)\\n\\nWhether you\'re a developer building the next generation of research tools, an academic seeking deeper insights, or a business professional needing comprehensive analysis, GPT Researcher\'s deep research capabilities offer an accessible, powerful solution that rivals - and in many ways exceeds - the offerings from major AI companies.\\n\\nThe future of AI-powered research is here, and it\'s open source. \ud83c\udf89\\n\\nHappy researching!"},{"id":"gptr-hybrid","metadata":{"permalink":"/blog/gptr-hybrid","source":"@site/blog/2024-09-7-hybrid-research/index.md","title":"The Future of Research is Hybrid","description":"Hyrbrid Research with GPT Researcher","date":"2024-09-07T00:00:00.000Z","formattedDate":"September 7, 2024","tags":[{"label":"hybrid-research","permalink":"/blog/tags/hybrid-research"},{"label":"gpt-researcher","permalink":"/blog/tags/gpt-researcher"},{"label":"langchain","permalink":"/blog/tags/langchain"},{"label":"langgraph","permalink":"/blog/tags/langgraph"},{"label":"tavily","permalink":"/blog/tags/tavily"}],"readingTime":7.545,"truncated":false,"authors":[{"name":"Assaf Elovic","title":"Creator @ GPT Researcher and Tavily","url":"https://github.com/assafelovic","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocJtrLku69VG_2Y0sJa5mt66gIGNaEBX5r_mgE6CRPEb7A=s96-c","key":"assafe"}],"prevItem":{"title":"Introducing Deep Research: The Open Source Alternative","permalink":"/blog/2025/02/26/deep-research"},"nextItem":{"title":"How to Build the Ultimate Research Multi-Agent Assistant","permalink":"/blog/gptr-langgraph"}},"content":"![Hyrbrid Research with GPT Researcher](https://miro.medium.com/v2/resize:fit:1400/1*MaauY1ecsD05nL8JqW0Zdg.jpeg)\\n\\nOver the past few years, we\'ve seen an explosion of new AI tools designed to disrupt research. Some, like [ChatPDF](https://www.chatpdf.com/) and [Consensus](https://consensus.app), focus on extracting insights from documents. Others, such as [Perplexity](https://www.perplexity.ai/), excel at scouring the web for information. But here\'s the thing: none of these tools combine both web and local document search within a single contextual research pipeline.\\n\\nThis is why I\'m excited to introduce the latest advancements of **[GPT Researcher](https://gptr.dev)** \u2014 now able to conduct hybrid research on any given task and documents.\\n\\nWeb driven research often lacks specific context, risks information overload, and may include outdated or unreliable data. On the flip side, local driven research is limited to historical data and existing knowledge, potentially creating organizational echo chambers and missing out on crucial market trends or competitor moves. Both approaches, when used in isolation, can lead to incomplete or biased insights, hampering your ability to make fully informed decisions.\\n\\nToday, we\'re going to change the game. By the end of this guide, you\'ll learn how to conduct hybrid research that combines the best of both worlds \u2014 web and local \u2014 enabling you to conduct more thorough, relevant, and insightful research.\\n\\n## Why Hybrid Research Works Better\\n\\nBy combining web and local sources, hybrid research addresses these limitations and offers several key advantages:\\n\\n1. **Grounded context**: Local documents provide a foundation of verified, organization specific information. This grounds the research in established knowledge, reducing the risk of straying from core concepts or misinterpreting industry specific terminology.\\n   \\n   *Example*: A pharmaceutical company researching a new drug development opportunity can use its internal research papers and clinical trial data as a base, then supplement this with the latest published studies and regulatory updates from the web.\\n\\n2. **Enhanced accuracy**: Web sources offer up-to-date information, while local documents provide historical context. This combination allows for more accurate trend analysis and decision-making.\\n   \\n   *Example*: A financial services firm analyzing market trends can combine their historical trading data with real-time market news and social media sentiment analysis to make more informed investment decisions.\\n\\n3. **Reduced bias**: By drawing from both web and local sources, we mitigate the risk of bias that might be present in either source alone.\\n   \\n   *Example*: A tech company evaluating its product roadmap can balance internal feature requests and usage data with external customer reviews and competitor analysis, ensuring a well-rounded perspective.\\n\\n4. **Improved planning and reasoning**: LLMs can leverage the context from local documents to better plan their web research strategies and reason about the information they find online.\\n   \\n   *Example*: An AI-powered market research tool can use a company\'s past campaign data to guide its web search for current marketing trends, resulting in more relevant and actionable insights.\\n\\n5. **Customized insights**: Hybrid research allows for the integration of proprietary information with public data, leading to unique, organization-specific insights.\\n   \\n   *Example*: A retail chain can combine its sales data with web-scraped competitor pricing and economic indicators to optimize its pricing strategy in different regions.\\n\\nThese are just a few examples for business use cases that can leverage hybrid research, but enough with the small talk \u2014 let\'s build!\\n\\n## Building the Hybrid Research Assistant\\n\\nBefore we dive into the details, it\'s worth noting that GPT Researcher has the capability to conduct hybrid research out of the box! However, to truly appreciate how this works and to give you a deeper understanding of the process, we\'re going to take a look under the hood.\\n\\n![GPT Researcher hybrid research](./gptr-hybrid.png)\\n\\nGPT Researcher conducts web research based on an auto-generated plan from local documents, as seen in the architecture above. It then retrieves relevant information from both local and web data for the final research report.\\n\\nWe\'ll explore how local documents are processed using LangChain, which is a key component of GPT Researcher\'s document handling. Then, we\'ll show you how to leverage GPT Researcher to conduct hybrid research, combining the advantages of web search with your local document knowledge base.\\n\\n### Processing Local Documents with Langchain\\n\\nLangChain provides a variety of document loaders that allow us to process different file types. This flexibility is crucial when dealing with diverse local documents. Here\'s how to set it up:\\n\\n```python\\nfrom langchain_community.document_loaders import (\\n    PyMuPDFLoader, \\n    TextLoader, \\n    UnstructuredCSVLoader, \\n    UnstructuredExcelLoader,\\n    UnstructuredMarkdownLoader, \\n    UnstructuredPowerPointLoader,\\n    UnstructuredWordDocumentLoader\\n)\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.vectorstores import Chroma\\n\\ndef load_local_documents(file_paths):\\n    documents = []\\n    for file_path in file_paths:\\n        if file_path.endswith(\'.pdf\'):\\n            loader = PyMuPDFLoader(file_path)\\n        elif file_path.endswith(\'.txt\'):\\n            loader = TextLoader(file_path)\\n        elif file_path.endswith(\'.csv\'):\\n            loader = UnstructuredCSVLoader(file_path)\\n        elif file_path.endswith(\'.xlsx\'):\\n            loader = UnstructuredExcelLoader(file_path)\\n        elif file_path.endswith(\'.md\'):\\n            loader = UnstructuredMarkdownLoader(file_path)\\n        elif file_path.endswith(\'.pptx\'):\\n            loader = UnstructuredPowerPointLoader(file_path)\\n        elif file_path.endswith(\'.docx\'):\\n            loader = UnstructuredWordDocumentLoader(file_path)\\n        else:\\n            raise ValueError(f\\"Unsupported file type: {file_path}\\")\\n        \\n        documents.extend(loader.load())\\n    \\n    return documents\\n\\n# Use the function to load your local documents\\nlocal_docs = load_local_documents([\'company_report.pdf\', \'meeting_notes.docx\', \'data.csv\'])\\n\\n# Split the documents into smaller chunks for more efficient processing\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nsplits = text_splitter.split_documents(local_docs)\\n\\n# Create embeddings and store them in a vector database for quick retrieval\\nembeddings = OpenAIEmbeddings()\\nvectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\\n\\n# Example of how to perform a similarity search\\nquery = \\"What were the key points from our last strategy meeting?\\"\\nrelevant_docs = vectorstore.similarity_search(query, k=3)\\n\\nfor doc in relevant_docs:\\n    print(doc.page_content)\\n```\\n\\n### Conducting Web Research with GPT Researcher\\n\\nNow that we\'ve learned how to work with local documents, let\'s take a quick look at how GPT Researcher works under the hood:\\n\\n![GPT Researcher Architecture](https://miro.medium.com/v2/resize:fit:1400/1*yFtT43N0GxL0TMKvjtYjug.png)\\n\\nAs seen above, GPT Researcher creates a research plan based on the given task by generating potential research queries that can collectively provide an objective and broad overview of the topic. Once these queries are generated, GPT Researcher uses a search engine like Tavily to find relevant results. Each scraped result is then saved in a vector database. Finally, the top k chunks most related to the research task are retrieved to generate a final research report.\\n\\nGPT Researcher supports hybrid research, which involves an additional step of chunking local documents (implemented using Langchain) before retrieving the most related information. After numerous evaluations conducted by the community, we\'ve found that hybrid research improved the correctness of final results by over 40%!\\n\\n### Running the Hybrid Research with GPT Researcher\\n\\nNow that you have a better understanding of how hybrid research works, let\'s demonstrate how easy this can be achieved with GPT Researcher.\\n\\n#### Step 1: Install GPT Researcher with PIP\\n\\n```bash\\npip install gpt-researcher\\n```\\n\\n#### Step 2: Setting up the environment\\n\\nWe will run GPT Researcher with OpenAI as the LLM vendor and Tavily as the search engine. You\'ll need to obtain API keys for both before moving forward. Then, export the environment variables in your CLI as follows:\\n\\n```bash\\nexport OPENAI_API_KEY={your-openai-key}\\nexport TAVILY_API_KEY={your-tavily-key}\\n```\\n\\n#### Step 3: Initialize GPT Researcher with hybrid research configuration\\n\\nGPT Researcher can be easily initialized with params that signal it to run a hybrid research. You can conduct many forms of research, head to the documentation page to learn more.\\n\\nTo get GPT Researcher to run a hybrid research, you need to include all relevant files in my-docs directory (create it if it doesn\'t exist), and set the instance report_source to \\"hybrid\\" as seen below. Once the report source is set to hybrid, GPT Researcher will look for existing documents in the my-docs directory and include them in the research. If no documents exist, it will ignore it.\\n\\n```python\\nfrom gpt_researcher import GPTResearcher\\nimport asyncio\\n\\nasync def get_research_report(query: str, report_type: str, report_source: str) -> str:\\n    researcher = GPTResearcher(query=query, report_type=report_type, report_source=report_source)\\n    research = await researcher.conduct_research()\\n    report = await researcher.write_report()\\n    return report\\n    \\nif __name__ == \\"__main__\\":\\n    query = \\"How does our product roadmap compare to emerging market trends in our industry?\\"\\n    report_source = \\"hybrid\\"\\n\\n    report = asyncio.run(get_research_report(query=query, report_type=\\"research_report\\", report_source=report_source))\\n    print(report)\\n```\\n\\nAs seen above, we can run the research on the following example:\\n\\n- Research task: \\"How does our product roadmap compare to emerging market trends in our industry?\\"\\n- Web: Current market trends, competitor announcements, and industry forecasts\\n- Local: Internal product roadmap documents and feature prioritization lists\\n\\nAfter various community evaluations we\'ve found that the results of this research improve quality and correctness of research by over 40% and remove hallucinations by 50%. Moreover as stated above, local information helps the LLM improve planning reasoning allowing it to make better decisions and researching more relevant web sources.\\n\\nBut wait, there\'s more! GPT Researcher also includes a sleek front-end app using NextJS and Tailwind. To learn how to get it running check out the documentation page. You can easily use drag and drop for documents to run hybrid research.\\n\\n## Conclusion\\n\\nHybrid research represents a significant advancement in data gathering and decision making. By leveraging tools like [GPT Researcher](https://gptr.dev), teams can now conduct more comprehensive, context-aware, and actionable research. This approach addresses the limitations of using web or local sources in isolation, offering benefits such as grounded context, enhanced accuracy, reduced bias, improved planning and reasoning, and customized insights.\\n\\nThe automation of hybrid research can enable teams to make faster, more data-driven decisions, ultimately enhancing productivity and offering a competitive advantage in analyzing an expanding pool of unstructured and dynamic information."},{"id":"gptr-langgraph","metadata":{"permalink":"/blog/gptr-langgraph","source":"@site/blog/2024-05-19-gptr-langgraph/index.md","title":"How to Build the Ultimate Research Multi-Agent Assistant","description":"Header","date":"2024-05-19T00:00:00.000Z","formattedDate":"May 19, 2024","tags":[{"label":"multi-skills","permalink":"/blog/tags/multi-skills"},{"label":"gpt-researcher","permalink":"/blog/tags/gpt-researcher"},{"label":"langchain","permalink":"/blog/tags/langchain"},{"label":"langgraph","permalink":"/blog/tags/langgraph"}],"readingTime":9.76,"truncated":false,"authors":[{"name":"Assaf Elovic","title":"Creator @ GPT Researcher and Tavily","url":"https://github.com/assafelovic","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocJtrLku69VG_2Y0sJa5mt66gIGNaEBX5r_mgE6CRPEb7A=s96-c","key":"assafe"}],"prevItem":{"title":"The Future of Research is Hybrid","permalink":"/blog/gptr-hybrid"},"nextItem":{"title":"How to build an OpenAI Assistant with Internet access","permalink":"/blog/building-openai-assistant"}},"content":"![Header](./blog-langgraph.jpeg)\\n# Introducing the GPT Researcher Multi-Agent Assistant\\n### Learn how to build an autonomous research assistant using LangGraph with a team of specialized AI agents\\n\\nIt has only been a year since the initial release of GPT Researcher, but methods for building, testing, and deploying AI agents have already evolved significantly. That\u2019s just the nature and speed of the current AI progress. What started as simple zero-shot or few-shot prompting, has quickly evolved to agent function calling, RAG and now finally agentic workflows (aka \u201cflow engineering\u201d).\\n\\nAndrew Ng has [recently stated](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/), \u201cI think AI agent workflows will drive massive AI progress this year \u2014 perhaps even more than the next generation of foundation models. This is an important trend, and I urge everyone who works in AI to pay attention to it.\u201d\\n\\nIn this article you will learn why multi-agent workflows are the current best standard and how to build the optimal autonomous research multi-agent assistant using LangGraph.\\n\\nTo skip this tutorial, feel free to check out the Github repo of [GPT Researcher x LangGraph](https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents).\\n\\n## Introducing LangGraph\\nLangGraph is an extension of LangChain aimed at creating agent and multi-agent flows. It adds in the ability to create cyclical flows and comes with memory built in \u2014 both important attributes for creating agents.\\n\\nLangGraph provides developers with a high degree of controllability and is important for creating custom agents and flows. Nearly all agents in production are customized towards the specific use case they are trying solve. LangGraph gives you the flexibility to create arbitrary customized agents, while providing an intuitive developer experience for doing so.\\n\\nEnough with the smalltalk, let\u2019s start building!\\n\\n## Building the Ultimate Autonomous Research Agent\\nBy leveraging LangGraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. Having every agent focus and specialize only a specific skill, allows for better separation of concerns, customizability, and further development at scale as the project grows.\\n\\nInspired by the recent STORM paper, this example showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication. This example will also leverage the leading autonomous research agent GPT Researcher.\\n\\n### The Research Agent Team\\nThe research team consists of seven LLM agents:\\n\\n* **Chief Editor** \u2014 Oversees the research process and manages the team. This is the \u201cmaster\u201d agent that coordinates the other agents using LangGraph. This agent acts as the main LangGraph interface.\\n* **GPT Researcher** \u2014 A specialized autonomous agent that conducts in depth research on a given topic.\\n* **Editor** \u2014 Responsible for planning the research outline and structure.\\n* **Reviewer** \u2014 Validates the correctness of the research results given a set of criteria.\\n* **Reviser** \u2014 Revises the research results based on the feedback from the reviewer.\\n* **Writer** \u2014 Responsible for compiling and writing the final report.\\n* **Publisher** \u2014 Responsible for publishing the final report in various formats.\\n\\n### Architecture\\nAs seen below, the automation process is based on the following stages: Planning the research, data collection and analysis, review and revision, writing the report and finally publication:\\n\\n![Architecture](./architecture.jpeg)\\n\\nMore specifically the process is as follows:\\n\\n* **Browser (gpt-researcher)** \u2014 Browses the internet for initial research based on the given research task. This step is crucial for LLMs to plan the research process based on up to date and relevant information, and not rely solely on pre-trained data for a given task or topic.\\n* **Editor** \u2014 Plans the report outline and structure based on the initial research. The Editor is also responsible for triggering the parallel research tasks based on the planned outline.\\n* For each outline topic (in parallel):\\n  * **Researcher (gpt-researcher)** \u2014 Runs an in depth research on the subtopics and writes a draft. This agent leverages the GPT Researcher Python package under the hood, for optimized, in depth and factual research report.\\n  * **Reviewer** \u2014 Validates the correctness of the draft given a set of guidelines and provides feedback to the reviser (if any).\\n  * **Reviser** \u2014 Revises the draft until it is satisfactory based on the reviewer feedback.\\n* **Writer** \u2014 Compiles and writes the final report including an introduction, conclusion and references section from the given research findings.\\n* **Publisher** \u2014 Publishes the final report to multi formats such as PDF, Docx, Markdown, etc.\\n\\n* We will not dive into all the code since there\u2019s a lot of it, but focus mostly on the interesting parts I\u2019ve found valuable to share.\\n\\n## Define the Graph State\\nOne of my favorite features with LangGraph is state management. States in LangGraph are facilitated through a structured approach where developers define a GraphState that encapsulates the entire state of the application. Each node in the graph can modify this state, allowing for dynamic responses based on the evolving context of the interaction.\\n\\nLike in every start of a technical design, considering the data schema throughout the application is key. In this case we\u2019ll define a ResearchState like so:\\n\\n```python\\nclass ResearchState(TypedDict):\\n    task: dict\\n    initial_research: str\\n    sections: List[str]\\n    research_data: List[dict]\\n    # Report layout\\n    title: str\\n    headers: dict\\n    date: str\\n    table_of_contents: str\\n    introduction: str\\n    conclusion: str\\n    sources: List[str]\\n    report: str\\n```\\n\\nAs seen above, the state is divided into two main areas: the research task and the report layout content. As data circulates through the graph agents, each agent will, in turn, generate new data based on the existing state and update it for subsequent processing further down the graph with other agents.\\n\\nWe can then initialize the graph with the following:\\n\\n\\n```python\\nfrom langgraph.graph import StateGraph\\nworkflow = StateGraph(ResearchState)\\n```\\n\\nInitializing the graph with LangGraph\\nAs stated above, one of the great things about multi-agent development is building each agent to have specialized and scoped skills. Let\u2019s take an example of the Researcher agent using GPT Researcher python package:\\n\\n```python\\nfrom gpt_researcher import GPTResearcher\\n\\nclass ResearchAgent:\\n    def __init__(self):\\n        pass\\n  \\n    async def research(self, query: str):\\n        # Initialize the researcher\\n        researcher = GPTResearcher(parent_query=parent_query, query=query, report_type=research_report, config_path=None)\\n        # Conduct research on the given query\\n        await researcher.conduct_research()\\n        # Write the report\\n        report = await researcher.write_report()\\n  \\n        return report\\n```\\n\\nAs you can see above, we\u2019ve created an instance of the Research agent. Now let\u2019s assume we\u2019ve done the same for each of the team\u2019s agent. After creating all of the agents, we\u2019d initialize the graph with LangGraph:\\n\\n```python\\ndef init_research_team(self):\\n    # Initialize skills\\n    editor_agent = EditorAgent(self.task)\\n    research_agent = ResearchAgent()\\n    writer_agent = WriterAgent()\\n    publisher_agent = PublisherAgent(self.output_dir)\\n    \\n    # Define a Langchain StateGraph with the ResearchState\\n    workflow = StateGraph(ResearchState)\\n    \\n    # Add nodes for each agent\\n    workflow.add_node(\\"browser\\", research_agent.run_initial_research)\\n    workflow.add_node(\\"planner\\", editor_agent.plan_research)\\n    workflow.add_node(\\"researcher\\", editor_agent.run_parallel_research)\\n    workflow.add_node(\\"writer\\", writer_agent.run)\\n    workflow.add_node(\\"publisher\\", publisher_agent.run)\\n    \\n    workflow.add_edge(\'browser\', \'planner\')\\n    workflow.add_edge(\'planner\', \'researcher\')\\n    workflow.add_edge(\'researcher\', \'writer\')\\n    workflow.add_edge(\'writer\', \'publisher\')\\n    \\n    # set up start and end nodes\\n    workflow.set_entry_point(\\"browser\\")\\n    workflow.add_edge(\'publisher\', END)\\n    \\n    return workflow\\n```\\n\\nAs seen above, creating the LangGraph graph is very straight forward and consists of three main functions: add_node, add_edge and set_entry_point. With these main functions you can first add the nodes to the graph, connect the edges and finally set the starting point.\\n\\nFocus check: If you\u2019ve been following the code and architecture properly, you\u2019ll notice that the Reviewer and Reviser agents are missing in the initialization above. Let\u2019s dive into it!\\n\\n## A Graph within a Graph to support stateful Parallelization\\nThis was the most exciting part of my experience working with LangGraph! One exciting feature of this autonomous assistant is having a parallel run for each research task, that would be reviewed and revised based on a set of predefined guidelines.\\n\\nKnowing how to leverage parallel work within a process is key for optimizing speed. But how would you trigger parallel agent work if all agents report to the same state? This can cause race conditions and inconsistencies in the final data report. To solve this, you can create a sub graph, that would be triggered from the main LangGraph instance. This sub graph would hold its own state for each parallel run, and that would solve the issues that were raised.\\n\\nAs we\u2019ve done before, let\u2019s define the LangGraph state and its agents. Since this sub graph basically reviews and revises a research draft, we\u2019ll define the state with draft information:\\n\\n```python\\nclass DraftState(TypedDict):\\n    task: dict\\n    topic: str\\n    draft: dict\\n    review: str\\n    revision_notes: str\\n```\\n\\nAs seen in the DraftState, we mostly care about the topic discussed, and the reviewer and revision notes as they communicate between each other to finalize the subtopic research report. To create the circular condition we\u2019ll take advantage of the last important piece of LangGraph which is conditional edges:\\n\\n```python\\nasync def run_parallel_research(self, research_state: dict):\\n    workflow = StateGraph(DraftState)\\n    \\n    workflow.add_node(\\"researcher\\", research_agent.run_depth_research)\\n    workflow.add_node(\\"reviewer\\", reviewer_agent.run)\\n    workflow.add_node(\\"reviser\\", reviser_agent.run)\\n    \\n    # set up edges researcher->reviewer->reviser->reviewer...\\n    workflow.set_entry_point(\\"researcher\\")\\n    workflow.add_edge(\'researcher\', \'reviewer\')\\n    workflow.add_edge(\'reviser\', \'reviewer\')\\n    workflow.add_conditional_edges(\'reviewer\',\\n                                   (lambda draft: \\"accept\\" if draft[\'review\'] is None else \\"revise\\"),\\n                                   {\\"accept\\": END, \\"revise\\": \\"reviser\\"})\\n```\\n\\nBy defining the conditional edges, the graph would direct to reviser if there exists review notes by the reviewer, or the cycle would end with the final draft. If you go back to the main graph we\u2019ve built, you\u2019ll see that this parallel work is under a node named \u201cresearcher\u201d called by ChiefEditor agent.\\n\\nRunning the Research Assistant\\nAfter finalizing the agents, states and graphs, it\u2019s time to run our research assistant! To make it easier to customize, the assistant runs with a given task.json file:\\n\\n```json\\n{\\n  \\"query\\": \\"Is AI in a hype cycle?\\",\\n  \\"max_sections\\": 3,\\n  \\"publish_formats\\": {\\n    \\"markdown\\": true,\\n    \\"pdf\\": true,\\n    \\"docx\\": true\\n  },\\n  \\"follow_guidelines\\": false,\\n  \\"model\\": \\"gpt-4-turbo\\",\\n  \\"guidelines\\": [\\n    \\"The report MUST be written in APA format\\",\\n    \\"Each sub section MUST include supporting sources using hyperlinks. If none exist, erase the sub section or rewrite it to be a part of the previous section\\",\\n    \\"The report MUST be written in spanish\\"\\n  ]\\n}\\n```\\n\\nThe task object is pretty self explanatory, however please notice that follow_guidelines if false would cause the graph to ignore the revision step and defined guidelines. Also, the max_sections field defines how many subheaders to research for. Having less will generate a shorter report.\\n\\nRunning the assistant will result in a final research report in formats such as Markdown, PDF and Docx.\\n\\nTo download and run the example check out the GPT Researcher x LangGraph [open source page](https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents).\\n\\n## What\u2019s Next?\\nGoing forward, there are super exciting things to think about. Human in the loop is key for optimized AI experiences. Having a human help the assistant revise and focus on just the right research plan, topics and outline, would enhance the overall quality and experience. Also generally, aiming for relying on human intervention throughout the AI flow ensures correctness, sense of control and deterministic results. Happy to see that LangGraph already supports this out of the box as seen here.\\n\\nIn addition, having support for research about both web and local data would be key for many types of business and personal use cases.\\n\\nLastly, more efforts can be done to improve the quality of retrieved sources and making sure the final report is built in the optimal storyline.\\n\\nA step forward in LangGraph and multi-agent collaboration in a whole would be where assistants can plan and generate graphs dynamically based on given tasks. This vision would allow assistants to choose only a subset of agents for a given task and plan their strategy based on the graph fundamentals as presented in this article and open a whole new world of possibilities. Given the pace of innovation in the AI space, it won\u2019t be long before a new disruptive version of GPT Researcher is launched. Looking forward to what the future brings!\\n\\nTo keep track of this project\u2019s ongoing progress and updates please join our Discord community. And as always, if you have any feedback or further questions, please comment below!"},{"id":"building-openai-assistant","metadata":{"permalink":"/blog/building-openai-assistant","source":"@site/blog/2023-11-12-openai-assistant/index.md","title":"How to build an OpenAI Assistant with Internet access","description":"OpenAI has done it again with a groundbreaking DevDay showcasing some of the latest improvements to the OpenAI suite of tools, products and services. One major release was the new Assistants API that makes it easier for developers to build their own assistive AI apps that have goals and can call models and tools.","date":"2023-11-12T00:00:00.000Z","formattedDate":"November 12, 2023","tags":[{"label":"tavily","permalink":"/blog/tags/tavily"},{"label":"search-api","permalink":"/blog/tags/search-api"},{"label":"openai","permalink":"/blog/tags/openai"},{"label":"assistant-api","permalink":"/blog/tags/assistant-api"}],"readingTime":5.855,"truncated":false,"authors":[{"name":"Assaf Elovic","title":"Creator @ GPT Researcher and Tavily","url":"https://github.com/assafelovic","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocJtrLku69VG_2Y0sJa5mt66gIGNaEBX5r_mgE6CRPEb7A=s96-c","key":"assafe"}],"prevItem":{"title":"How to Build the Ultimate Research Multi-Agent Assistant","permalink":"/blog/gptr-langgraph"},"nextItem":{"title":"How we built GPT Researcher","permalink":"/blog/building-gpt-researcher"}},"content":"OpenAI has done it again with a [groundbreaking DevDay](https://openai.com/blog/new-models-and-developer-products-announced-at-devday) showcasing some of the latest improvements to the OpenAI suite of tools, products and services. One major release was the new [Assistants API](https://platform.openai.com/docs/assistants/overview) that makes it easier for developers to build their own assistive AI apps that have goals and can call models and tools.\\n\\nThe new Assistants API currently supports three types of tools: Code Interpreter, Retrieval, and Function calling. Although you might expect the Retrieval tool to support online information retrieval (such as search APIs or as ChatGPT plugins), it only supports raw data for now such as text or CSV files.\\n\\nThis blog will demonstrate how to leverage the latest Assistants API with online information using the function calling tool.\\n\\nTo skip the tutorial below, feel free to check out the full [Github Gist here](https://gist.github.com/assafelovic/579822cd42d52d80db1e1c1ff82ffffd).\\n\\nAt a high level, a typical integration of the Assistants API has the following steps:\\n\\n- Create an [Assistant](https://platform.openai.com/docs/api-reference/assistants/createAssistant) in the API by defining its custom instructions and picking a model. If helpful, enable tools like Code Interpreter, Retrieval, and Function calling.\\n- Create a [Thread](https://platform.openai.com/docs/api-reference/threads) when a user starts a conversation.\\n- Add [Messages](https://platform.openai.com/docs/api-reference/messages) to the Thread as the user ask questions.\\n- [Run](https://platform.openai.com/docs/api-reference/runs) the Assistant on the Thread to trigger responses. This automatically calls the relevant tools.\\n\\nAs you can see below, an Assistant object includes Threads for storing and handling conversation sessions between the assistant and users, and Run for invocation of an Assistant on a Thread.\\n\\n![OpenAI Assistant Object](./diagram-assistant.jpeg)\\n\\nLet\u2019s go ahead and implement these steps one by one! For the example, we will build a finance GPT that can provide insights about financial questions. We will use the [OpenAI Python SDK v1.2](https://github.com/openai/openai-python/tree/main#installation) and [Tavily Search API](https://tavily.com).\\n\\nFirst things first, let\u2019s define the assistant\u2019s instructions:\\n\\n```python\\nassistant_prompt_instruction = \\"\\"\\"You are a finance expert. \\nYour goal is to provide answers based on information from the internet. \\nYou must use the provided Tavily search API function to find relevant online information. \\nYou should never use your own knowledge to answer questions.\\nPlease include relevant url sources in the end of your answers.\\n\\"\\"\\"\\n```\\nNext, let\u2019s finalize step 1 and create an assistant using the latest [GPT-4 Turbo model](https://github.com/openai/openai-python/tree/main#installation) (128K context), and the call function using the [Tavily web search API](https://tavily.com/):\\n\\n```python\\n# Create an assistant\\nassistant = client.beta.assistants.create(\\n    instructions=assistant_prompt_instruction,\\n    model=\\"gpt-4-1106-preview\\",\\n    tools=[{\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"tavily_search\\",\\n            \\"description\\": \\"Get information on recent events from the web.\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"query\\": {\\"type\\": \\"string\\", \\"description\\": \\"The search query to use. For example: \'Latest news on Nvidia stock performance\'\\"},\\n                },\\n                \\"required\\": [\\"query\\"]\\n            }\\n        }\\n    }]\\n)\\n```\\n\\nStep 2+3 are quite straight forward, we\u2019ll initiate a new thread and update it with a user message:\\n\\n```python\\nthread = client.beta.threads.create()\\nuser_input = input(\\"You: \\")\\nmessage = client.beta.threads.messages.create(\\n    thread_id=thread.id,\\n    role=\\"user\\",\\n    content=user_input,\\n)\\n```\\n\\nFinally, we\u2019ll run the assistant on the thread to trigger the function call and get the response:\\n\\n```python\\nrun = client.beta.threads.runs.create(\\n    thread_id=thread.id,\\n    assistant_id=assistant_id,\\n)\\n```\\n\\nSo far so good! But this is where it gets a bit messy. Unlike with the regular GPT APIs, the Assistants API doesn\u2019t return a synchronous response, but returns a status. This allows for asynchronous operations across assistants, but requires more overhead for fetching statuses and dealing with each manually.\\n\\n![Status Diagram](./diagram-1.png)\\n\\nTo manage this status lifecycle, let\u2019s build a function that can be reused and handles waiting for various statuses (such as \u2018requires_action\u2019):\\n\\n```python\\n# Function to wait for a run to complete\\ndef wait_for_run_completion(thread_id, run_id):\\n    while True:\\n        time.sleep(1)\\n        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\\n        print(f\\"Current run status: {run.status}\\")\\n        if run.status in [\'completed\', \'failed\', \'requires_action\']:\\n            return run\\n```\\n\\nThis function will sleep as long as the run has not been finalized such as in cases where it\u2019s completed or requires an action from a function call.\\n\\nWe\u2019re almost there! Lastly, let\u2019s take care of when the assistant wants to call the web search API:\\n\\n```python\\n# Function to handle tool output submission\\ndef submit_tool_outputs(thread_id, run_id, tools_to_call):\\n    tool_output_array = []\\n    for tool in tools_to_call:\\n        output = None\\n        tool_call_id = tool.id\\n        function_name = tool.function.name\\n        function_args = tool.function.arguments\\n\\n        if function_name == \\"tavily_search\\":\\n            output = tavily_search(query=json.loads(function_args)[\\"query\\"])\\n\\n        if output:\\n            tool_output_array.append({\\"tool_call_id\\": tool_call_id, \\"output\\": output})\\n\\n    return client.beta.threads.runs.submit_tool_outputs(\\n        thread_id=thread_id,\\n        run_id=run_id,\\n        tool_outputs=tool_output_array\\n    )\\n```\\n\\nAs seen above, if the assistant has reasoned that a function call should trigger, we extract the given required function params and pass back to the runnable thread. We catch this status and call our functions as seen below:\\n\\n```python\\nif run.status == \'requires_action\':\\n    run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\\n    run = wait_for_run_completion(thread.id, run.id)\\n```\\n\\nThat\u2019s it! We now have a working OpenAI Assistant that can be used to answer financial questions using real time online information. Below is the full runnable code:\\n\\n```python\\nimport os\\nimport json\\nimport time\\nfrom openai import OpenAI\\nfrom tavily import TavilyClient\\n\\n# Initialize clients with API keys\\nclient = OpenAI(api_key=os.environ[\\"OPENAI_API_KEY\\"])\\ntavily_client = TavilyClient(api_key=os.environ[\\"TAVILY_API_KEY\\"])\\n\\nassistant_prompt_instruction = \\"\\"\\"You are a finance expert. \\nYour goal is to provide answers based on information from the internet. \\nYou must use the provided Tavily search API function to find relevant online information. \\nYou should never use your own knowledge to answer questions.\\nPlease include relevant url sources in the end of your answers.\\n\\"\\"\\"\\n\\n# Function to perform a Tavily search\\ndef tavily_search(query):\\n    search_result = tavily_client.get_search_context(query, search_depth=\\"advanced\\", max_tokens=8000)\\n    return search_result\\n\\n# Function to wait for a run to complete\\ndef wait_for_run_completion(thread_id, run_id):\\n    while True:\\n        time.sleep(1)\\n        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\\n        print(f\\"Current run status: {run.status}\\")\\n        if run.status in [\'completed\', \'failed\', \'requires_action\']:\\n            return run\\n\\n# Function to handle tool output submission\\ndef submit_tool_outputs(thread_id, run_id, tools_to_call):\\n    tool_output_array = []\\n    for tool in tools_to_call:\\n        output = None\\n        tool_call_id = tool.id\\n        function_name = tool.function.name\\n        function_args = tool.function.arguments\\n\\n        if function_name == \\"tavily_search\\":\\n            output = tavily_search(query=json.loads(function_args)[\\"query\\"])\\n\\n        if output:\\n            tool_output_array.append({\\"tool_call_id\\": tool_call_id, \\"output\\": output})\\n\\n    return client.beta.threads.runs.submit_tool_outputs(\\n        thread_id=thread_id,\\n        run_id=run_id,\\n        tool_outputs=tool_output_array\\n    )\\n\\n# Function to print messages from a thread\\ndef print_messages_from_thread(thread_id):\\n    messages = client.beta.threads.messages.list(thread_id=thread_id)\\n    for msg in messages:\\n        print(f\\"{msg.role}: {msg.content[0].text.value}\\")\\n\\n# Create an assistant\\nassistant = client.beta.assistants.create(\\n    instructions=assistant_prompt_instruction,\\n    model=\\"gpt-4-1106-preview\\",\\n    tools=[{\\n        \\"type\\": \\"function\\",\\n        \\"function\\": {\\n            \\"name\\": \\"tavily_search\\",\\n            \\"description\\": \\"Get information on recent events from the web.\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"query\\": {\\"type\\": \\"string\\", \\"description\\": \\"The search query to use. For example: \'Latest news on Nvidia stock performance\'\\"},\\n                },\\n                \\"required\\": [\\"query\\"]\\n            }\\n        }\\n    }]\\n)\\nassistant_id = assistant.id\\nprint(f\\"Assistant ID: {assistant_id}\\")\\n\\n# Create a thread\\nthread = client.beta.threads.create()\\nprint(f\\"Thread: {thread}\\")\\n\\n# Ongoing conversation loop\\nwhile True:\\n    user_input = input(\\"You: \\")\\n    if user_input.lower() == \'exit\':\\n        break\\n\\n    # Create a message\\n    message = client.beta.threads.messages.create(\\n        thread_id=thread.id,\\n        role=\\"user\\",\\n        content=user_input,\\n    )\\n\\n    # Create a run\\n    run = client.beta.threads.runs.create(\\n        thread_id=thread.id,\\n        assistant_id=assistant_id,\\n    )\\n    print(f\\"Run ID: {run.id}\\")\\n\\n    # Wait for run to complete\\n    run = wait_for_run_completion(thread.id, run.id)\\n\\n    if run.status == \'failed\':\\n        print(run.error)\\n        continue\\n    elif run.status == \'requires_action\':\\n        run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\\n        run = wait_for_run_completion(thread.id, run.id)\\n\\n    # Print messages from the thread\\n    print_messages_from_thread(thread.id)\\n```\\n\\nThe assistant can be further customized and improved using additional retrieval information, OpenAI\u2019s coding interpreter and more. Also, you can go ahead and add more function tools to make the assistant even smarter.\\n\\nFeel free to drop a comment below if you have any further questions!"},{"id":"building-gpt-researcher","metadata":{"permalink":"/blog/building-gpt-researcher","source":"@site/blog/2023-09-22-gpt-researcher/index.md","title":"How we built GPT Researcher","description":"After AutoGPT was published, we immediately took it for a spin. The first use case that came to mind was autonomous online research. Forming objective conclusions for manual research tasks can take time, sometimes weeks, to find the right resources and information. Seeing how well AutoGPT created tasks and executed them got me thinking about the great potential of using AI to conduct comprehensive research and what it meant for the future of online research.","date":"2023-09-22T00:00:00.000Z","formattedDate":"September 22, 2023","tags":[{"label":"gpt-researcher","permalink":"/blog/tags/gpt-researcher"},{"label":"autonomous-agent","permalink":"/blog/tags/autonomous-agent"},{"label":"opensource","permalink":"/blog/tags/opensource"},{"label":"github","permalink":"/blog/tags/github"}],"readingTime":6.255,"truncated":false,"authors":[{"name":"Assaf Elovic","title":"Creator @ GPT Researcher and Tavily","url":"https://github.com/assafelovic","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocJtrLku69VG_2Y0sJa5mt66gIGNaEBX5r_mgE6CRPEb7A=s96-c","key":"assafe"}],"prevItem":{"title":"How to build an OpenAI Assistant with Internet access","permalink":"/blog/building-openai-assistant"}},"content":"After [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) was published, we immediately took it for a spin. The first use case that came to mind was autonomous online research. Forming objective conclusions for manual research tasks can take time, sometimes weeks, to find the right resources and information. Seeing how well AutoGPT created tasks and executed them got me thinking about the great potential of using AI to conduct comprehensive research and what it meant for the future of online research.\\n\\nBut the problem with AutoGPT was that it usually ran into never-ending loops, required human interference for almost every step, constantly lost track of its progress, and almost never actually completed the task.\\n\\nNonetheless, the information and context gathered during the research task were lost (such as keeping track of sources), and sometimes hallucinated.\\n\\nThe passion for leveraging AI for online research and the limitations I found put me on a mission to try and solve it while sharing my work with the world. This is when I created [GPT Researcher](https://github.com/assafelovic/gpt-researcher) \u2014 an open source autonomous agent for online comprehensive research.\\n\\nIn this article, we will share the steps that guided me toward the proposed solution.\\n\\n### Moving from infinite loops to deterministic results\\nThe first step in solving these issues was to seek a more deterministic solution that could ultimately guarantee completing any research task within a fixed time frame, without human interference.\\n\\nThis is when we stumbled upon the recent paper [Plan and Solve](https://arxiv.org/abs/2305.04091). The paper aims to provide a better solution for the challenges stated above. The idea is quite simple and consists of two components: first, devising a plan to divide the entire task into smaller subtasks and then carrying out the subtasks according to the plan.\\n\\n![Planner-Excutor-Model](./planner.jpeg)\\n\\nAs it relates to research, first create an outline of questions to research related to the task, and then deterministically execute an agent for every outline item. This approach eliminates the uncertainty in task completion by breaking the agent steps into a deterministic finite set of tasks. Once all tasks are completed, the agent concludes the research.\\n\\nFollowing this strategy has improved the reliability of completing research tasks to 100%. Now the challenge is, how to improve quality and speed?\\n\\n### Aiming for objective and unbiased results\\nThe biggest challenge with LLMs is the lack of factuality and unbiased responses caused by hallucinations and out-of-date training sets (GPT is currently trained on datasets from 2021). But the irony is that for research tasks, it is crucial to optimize for these exact two criteria: factuality and bias.\\n\\nTo tackle this challenges, we assumed the following:\\n\\n- Law of large numbers \u2014 More content will lead to less biased results. Especially if gathered properly.\\n- Leveraging LLMs for the summarization of factual information can significantly improve the overall better factuality of results.\\n\\nAfter experimenting with LLMs for quite some time, we can say that the areas where foundation models excel are in the summarization and rewriting of given content. So, in theory, if LLMs only review given content and summarize and rewrite it, potentially it would reduce hallucinations significantly.\\n\\nIn addition, assuming the given content is unbiased, or at least holds opinions and information from all sides of a topic, the rewritten result would also be unbiased. So how can content be unbiased? The [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). In other words, if enough sites that hold relevant information are scraped, the possibility of biased information reduces greatly. So the idea would be to scrape just enough sites together to form an objective opinion on any topic.\\n\\nGreat! Sounds like, for now, we have an idea for how to create both deterministic, factual, and unbiased results. But what about the speed problem?\\n\\n### Speeding up the research process\\nAnother issue with AutoGPT is that it works synchronously. The main idea of it is to create a list of tasks and then execute them one by one. So if, let\u2019s say, a research task requires visiting 20 sites, and each site takes around one minute to scrape and summarize, the overall research task would take a minimum of +20 minutes. That\u2019s assuming it ever stops. But what if we could parallelize agent work?\\n\\nBy levering Python libraries such as asyncio, the agent tasks have been optimized to work in parallel, thus significantly reducing the time to research.\\n\\n```python\\n# Create a list to hold the coroutine agent tasks\\ntasks = [async_browse(url, query, self.websocket) for url in await new_search_urls]\\n\\n# Gather the results as they become available\\nresponses = await asyncio.gather(*tasks, return_exceptions=True)\\n```\\n\\nIn the example above, we trigger scraping for all URLs in parallel, and only once all is done, continue with the task. Based on many tests, an average research task takes around three minutes (!!). That\u2019s 85% faster than AutoGPT.\\n\\n### Finalizing the research report\\nFinally, after aggregating as much information as possible about a given research task, the challenge is to write a comprehensive report about it.\\n\\nAfter experimenting with several OpenAI models and even open source, I\u2019ve concluded that the best results are currently achieved with GPT-4. The task is straightforward \u2014 provide GPT-4 as context with all the aggregated information, and ask it to write a detailed report about it given the original research task.\\n\\nThe prompt is as follows:\\n```commandline\\n\\"{research_summary}\\" Using the above information, answer the following question or topic: \\"{question}\\" in a detailed report \u2014 The report should focus on the answer to the question, should be well structured, informative, in depth, with facts and numbers if available, a minimum of 1,200 words and with markdown syntax and apa format. Write all source urls at the end of the report in apa format. You should write your report only based on the given information and nothing else.\\n```\\n\\nThe results are quite impressive, with some minor hallucinations in very few samples, but it\u2019s fair to assume that as GPT improves over time, results will only get better.\\n\\n### The final architecture\\nNow that we\u2019ve reviewed the necessary steps of GPT Researcher, let\u2019s break down the final architecture, as shown below:\\n\\n<div align=\\"center\\">\\n<img align=\\"center\\" height=\\"500\\" src=\\"https://cowriter-images.s3.amazonaws.com/architecture.png\\"/>\\n</div>\\n\\nMore specifically:\\n- Generate an outline of research questions that form an objective opinion on any given task.\\n- For each research question, trigger a crawler agent that scrapes online resources for information relevant to the given task.\\n- For each scraped resource, keep track, filter, and summarize only if it includes relevant information.\\n- Finally, aggregate all summarized sources and generate a final research report.\\n\\n### Going forward\\nThe future of online research automation is heading toward a major disruption. As AI continues to improve, it is only a matter of time before AI agents can perform comprehensive research tasks for any of our day-to-day needs. AI research can disrupt areas of finance, legal, academia, health, and retail, reducing our time for each research by 95% while optimizing for factual and unbiased reports within an influx and overload of ever-growing online information.\\n\\nImagine if an AI can eventually understand and analyze any form of online content \u2014 videos, images, graphs, tables, reviews, text, audio. And imagine if it could support and analyze hundreds of thousands of words of aggregated information within a single prompt. Even imagine that AI can eventually improve in reasoning and analysis, making it much more suitable for reaching new and innovative research conclusions. And that it can do all that in minutes, if not seconds.\\n\\nIt\u2019s all a matter of time and what [GPT Researcher](https://github.com/assafelovic/gpt-researcher) is all about."}]}')}}]);