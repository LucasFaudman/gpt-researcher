"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7845],{3957:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>c,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"gpt-researcher/context/data-ingestion","title":"Data Ingestion","description":"When you\'re dealing with a large amount of context data, you may want to start meditating upon a standalone process for data ingestion.","source":"@site/docs/gpt-researcher/context/data-ingestion.md","sourceDirName":"gpt-researcher/context","slug":"/gpt-researcher/context/data-ingestion","permalink":"/docs/gpt-researcher/context/data-ingestion","draft":false,"unlisted":false,"editUrl":"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/context/data-ingestion.md","tags":[],"version":"current","frontMatter":{},"sidebar":"docsSidebar","previous":{"title":"Vector Stores","permalink":"/docs/gpt-researcher/context/vector-stores"},"next":{"title":"All About Logs","permalink":"/docs/gpt-researcher/handling-logs/all-about-logs"}}');var r=t(4848),s=t(8453);const a={},c="Data Ingestion",i={},d=[{value:"Step 1: Transform your content into Langchain Documents",id:"step-1-transform-your-content-into-langchain-documents",level:3},{value:"Step 2: Insert your Langchain Documents into a Langchain VectorStore",id:"step-2-insert-your-langchain-documents-into-a-langchain-vectorstore",level:3},{value:"Step 3: Pass your Langchain Vectorstore into your GPTR report",id:"step-3-pass-your-langchain-vectorstore-into-your-gptr-report",level:3}];function l(e){const n={a:"a",code:"code",h1:"h1",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"data-ingestion",children:"Data Ingestion"})}),"\n",(0,r.jsx)(n.p,{children:"When you're dealing with a large amount of context data, you may want to start meditating upon a standalone process for data ingestion."}),"\n",(0,r.jsx)(n.p,{children:"Some signs that the system is telling you to move to a custom data ingestion process:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Your embedding model is hitting API rate limits"}),"\n",(0,r.jsx)(n.li,{children:"Your Langchain VectorStore's underlying database needs rate limiting"}),"\n",(0,r.jsx)(n.li,{children:"You sense you need to add custom pacing/throttling logic in your Python code"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["As mentioned in our ",(0,r.jsx)(n.a,{href:"https://www.youtube.com/watch?v=yRuduRCblbg",children:"YouTube Tutorial Series"}),", GPTR is using ",(0,r.jsx)(n.a,{href:"https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html",children:"Langchain Documents"})," and ",(0,r.jsx)(n.a,{href:"https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/",children:"Langchain VectorStores"})," under the hood."]}),"\n",(0,r.jsx)(n.p,{children:"These are 2 beautiful abstractions that make the GPTR architecture highly configurable."}),"\n",(0,r.jsx)(n.p,{children:"The current research flow, whether you're generating reports on web or local documents, is:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Step 1: transform your content (web results or local documents) into Langchain Documents\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Step 2: Insert your Langchain Documents into a Langchain VectorStore\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"Step 3: Pass your Langchain Vectorstore into your GPTR report ([more on that here](https://docs.gptr.dev/docs/gpt-researcher/context/vector-stores) and below)\n"})}),"\n",(0,r.jsx)(n.p,{children:"Code samples below:"}),"\n",(0,r.jsx)(n.p,{children:"Assuming your .env variables are like so:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"OPENAI_API_KEY={Your OpenAI API Key here}\nTAVILY_API_KEY={Your Tavily API Key here}\nPGVECTOR_CONNECTION_STRING=postgresql://username:password...\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Below is a custom data ingestion process that you can use to ingest your data into a Langchain VectorStore. See a ",(0,r.jsx)(n.a,{href:"https://github.com/assafelovic/gpt-researcher/pull/819#issue-2501632831",children:"full working example here"}),".\nIn this example, we're using a Postgres VectorStore to embed data of a Github Branch, but you can use ",(0,r.jsx)(n.a,{href:"https://python.langchain.com/v0.2/docs/integrations/vectorstores/",children:"any supported Langchain VectorStore"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Note that when you create the Langchain Documents, you should include as metadata the ",(0,r.jsx)(n.code,{children:"source"})," and ",(0,r.jsx)(n.code,{children:"title"})," fields in order for GPTR to leverage your Documents seamlessly. In the example below, we're splitting the documents list into chunks of 100 & then inserting 1 chunk at a time into the vector store."]}),"\n",(0,r.jsx)(n.h3,{id:"step-1-transform-your-content-into-langchain-documents",children:"Step 1: Transform your content into Langchain Documents"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain_core.documents import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nasync def transform_to_langchain_docs(self, directory_structure):\n    documents = []\n    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n    run_timestamp = datetime.utcnow().strftime(\'%Y%m%d%H%M%S\')\n\n    for file_name in directory_structure:\n        if not file_name.endswith(\'/\'):\n            try:\n                content = self.repo.get_contents(file_name, ref=self.branch_name)\n                try:\n                    decoded_content = base64.b64decode(content.content).decode()\n                except Exception as e:\n                    print(f"Error decoding content: {e}")\n                    print("the problematic file_name is", file_name)\n                    continue\n                print("file_name", file_name)\n                print("content", decoded_content)\n\n                # Split each document into smaller chunks\n                chunks = splitter.split_text(decoded_content)\n\n                # Extract metadata for each chunk\n                for index, chunk in enumerate(chunks):\n                    metadata = {\n                        "id": f"{run_timestamp}_{uuid4()}",  # Generate a unique UUID for each document\n                        "source": file_name,\n                        "title": file_name,\n                        "extension": os.path.splitext(file_name)[1],\n                        "file_path": file_name\n                    }\n                    document = Document(\n                        page_content=chunk,\n                        metadata=metadata\n                    )\n                    documents.append(document)\n\n            except Exception as e:\n                print(f"Error saving to vector store: {e}")\n                return None\n\n    await save_to_vector_store(documents)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-insert-your-langchain-documents-into-a-langchain-vectorstore",children:"Step 2: Insert your Langchain Documents into a Langchain VectorStore"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from langchain_postgres import PGVector\nfrom langchain_postgres.vectorstores import PGVector\nfrom sqlalchemy.ext.asyncio import create_async_engine\n\nfrom langchain_community.embeddings import OpenAIEmbeddings\n\nasync def save_to_vector_store(self, documents):\n    # The documents are already Document objects, so we don\'t need to convert them\n    embeddings = OpenAIEmbeddings()\n    # self.vector_store = FAISS.from_documents(documents, embeddings)\n    pgvector_connection_string = os.environ["PGVECTOR_CONNECTION_STRING"]\n\n    collection_name = "my_docs"\n\n    vector_store = PGVector(\n        embeddings=embeddings,\n        collection_name=collection_name,\n        connection=pgvector_connection_string,\n        use_jsonb=True\n    )\n\n    # for faiss\n    # self.vector_store = vector_store.add_documents(documents, ids=[doc.metadata["id"] for doc in documents])\n\n    # Split the documents list into chunks of 100\n    for i in range(0, len(documents), 100):\n        chunk = documents[i:i+100]\n        # Insert the chunk into the vector store\n        vector_store.add_documents(chunk, ids=[doc.metadata["id"] for doc in chunk])\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-pass-your-langchain-vectorstore-into-your-gptr-report",children:"Step 3: Pass your Langchain Vectorstore into your GPTR report"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'async_connection_string = pgvector_connection_string.replace("postgresql://", "postgresql+psycopg://")\n\n# Initialize the async engine with the psycopg3 driver\nasync_engine = create_async_engine(\n    async_connection_string,\n    echo=True\n)\n\nasync_vector_store = PGVector(\n    embeddings=embeddings,\n    collection_name=collection_name,\n    connection=async_engine,\n    use_jsonb=True\n)\n\n\nresearcher = GPTResearcher(\n    query=query,\n    report_type="research_report",\n    report_source="langchain_vectorstore",\n    vector_store=async_vector_store,\n)\nawait researcher.conduct_research()\nreport = await researcher.write_report()\n'})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>c});var o=t(6540);const r={},s=o.createContext(r);function a(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);