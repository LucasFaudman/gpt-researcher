"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6154],{3020:(e,n,r)=>{r.r(n),r.d(n,{contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var t=r(8168),a=(r(6540),r(5680));const i={sidebar_position:2},o="Advanced Usage",l={unversionedId:"gpt-researcher/mcp-server/advanced-usage",id:"gpt-researcher/mcp-server/advanced-usage",isDocsHomePage:!1,title:"Advanced Usage",description:"This guide covers advanced usage scenarios and configurations for the GPT Researcher MCP Server.",source:"@site/docs/gpt-researcher/mcp-server/advanced-usage.md",sourceDirName:"gpt-researcher/mcp-server",slug:"/gpt-researcher/mcp-server/advanced-usage",permalink:"/docs/gpt-researcher/mcp-server/advanced-usage",editUrl:"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/mcp-server/advanced-usage.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"docsSidebar",previous:{title:"Getting Started",permalink:"/docs/gpt-researcher/mcp-server/getting-started"},next:{title:"Integrating with Claude",permalink:"/docs/gpt-researcher/mcp-server/claude-integration"}},s=[{value:"Custom Configuration",id:"custom-configuration",children:[{value:"Environment Variables",id:"environment-variables",children:[],level:3},{value:"Server Configuration File",id:"server-configuration-file",children:[],level:3}],level:2},{value:"Integrating with Claude",id:"integrating-with-claude",children:[],level:2},{value:"Advanced Tool Usage",id:"advanced-tool-usage",children:[{value:"Conducting Deep Research",id:"conducting-deep-research",children:[],level:3},{value:"Customizing Report Generation",id:"customizing-report-generation",children:[],level:3}],level:2},{value:"Securing Your MCP Server",id:"securing-your-mcp-server",children:[],level:2},{value:"Deploying with Docker",id:"deploying-with-docker",children:[],level:2},{value:"Monitoring and Logging",id:"monitoring-and-logging",children:[],level:2},{value:"Extending Functionality",id:"extending-functionality",children:[],level:2},{value:"Troubleshooting Advanced Issues",id:"troubleshooting-advanced-issues",children:[{value:"Handling Rate Limits",id:"handling-rate-limits",children:[],level:3},{value:"Memory Management",id:"memory-management",children:[],level:3}],level:2},{value:"Next Steps",id:"next-steps",children:[],level:2}],c={toc:s},g="wrapper";function p(e){let{components:n,...r}=e;return(0,a.yg)(g,(0,t.A)({},c,r,{components:n,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"advanced-usage"},"Advanced Usage"),(0,a.yg)("p",null,"This guide covers advanced usage scenarios and configurations for the GPT Researcher MCP Server."),(0,a.yg)("h2",{id:"custom-configuration"},"Custom Configuration"),(0,a.yg)("p",null,"You can customize the MCP server behavior by modifying various configuration parameters:"),(0,a.yg)("h3",{id:"environment-variables"},"Environment Variables"),(0,a.yg)("p",null,"Create a ",(0,a.yg)("inlineCode",{parentName:"p"},".env")," file with additional configuration options:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"# Required API keys\nOPENAI_API_KEY=your_openai_api_key\nTAVILY_API_KEY=your_tavily_api_key\n\n# Optional configurations\nDEFAULT_MODEL=gpt-4-turbo  # Default LLM to use\nMAX_RESEARCH_DEPTH=5       # Maximum research depth\nENABLE_LOGGING=true        # Enable detailed logging\n")),(0,a.yg)("h3",{id:"server-configuration-file"},"Server Configuration File"),(0,a.yg)("p",null,"You can create a ",(0,a.yg)("inlineCode",{parentName:"p"},"config.json")," file to customize server behavior:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "host": "0.0.0.0",\n  "port": 8000,\n  "debug": false,\n  "timeout": 300,\n  "max_concurrent_requests": 10\n}\n')),(0,a.yg)("h2",{id:"integrating-with-claude"},"Integrating with Claude"),(0,a.yg)("p",null,"To integrate with Claude effectively:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Make sure your Claude model has MCP capabilities enabled"),(0,a.yg)("li",{parentName:"ol"},"Point Claude to the MCP server endpoint"),(0,a.yg)("li",{parentName:"ol"},"Use the appropriate prompts to guide Claude in using the research tools")),(0,a.yg)("p",null,"Example configuration for Claude:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-json"},'{\n  "tools": [\n    {\n      "name": "gptr-researcher",\n      "endpoint": "http://localhost:8000/mcp"\n    }\n  ]\n}\n')),(0,a.yg)("h2",{id:"advanced-tool-usage"},"Advanced Tool Usage"),(0,a.yg)("h3",{id:"conducting-deep-research"},"Conducting Deep Research"),(0,a.yg)("p",null,"For deeper research capabilities:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},'Use the conduct_research tool with these advanced parameters:\n{\n  "query": "quantum computing advancements 2024",\n  "depth": "deep",\n  "focus_areas": ["hardware", "algorithms", "applications"],\n  "timeline": "last 1 year"\n}\n')),(0,a.yg)("h3",{id:"customizing-report-generation"},"Customizing Report Generation"),(0,a.yg)("p",null,"The write_report tool accepts several customization options:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},'Use the write_report tool with:\n{\n  "style": "academic",\n  "format": "markdown",\n  "include_images": true,\n  "citation_style": "APA",\n  "executive_summary": true\n}\n')),(0,a.yg)("h2",{id:"securing-your-mcp-server"},"Securing Your MCP Server"),(0,a.yg)("p",null,"To secure your MCP server deployment:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("p",{parentName:"li"},"Add API key authentication:"),(0,a.yg)("pre",{parentName:"li"},(0,a.yg)("code",{parentName:"pre",className:"language-python"},'# Add to server.py\n@app.middleware("http")\nasync def verify_api_key(request, call_next):\n    api_key = request.headers.get("X-API-Key")\n    if api_key != os.getenv("MCP_API_KEY"):\n        return JSONResponse(status_code=401, content={"error": "Invalid API key"})\n    return await call_next(request)\n'))),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("p",{parentName:"li"},"Enable HTTPS:"),(0,a.yg)("pre",{parentName:"li"},(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"# Run with HTTPS\nuvicorn server:app --host 0.0.0.0 --port 8000 --ssl-keyfile=./key.pem --ssl-certfile=./cert.pem\n"))),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("p",{parentName:"li"},"Set up rate limiting:"),(0,a.yg)("pre",{parentName:"li"},(0,a.yg)("code",{parentName:"pre",className:"language-python"},'# Add rate limiting\nfrom fastapi import Depends, HTTPException\nfrom slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\n\nlimiter = Limiter(key_func=get_remote_address)\napp.state.limiter = limiter\napp.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n\n@app.post("/mcp")\n@limiter.limit("10/minute")\nasync def mcp_endpoint(request: Request, payload: dict):\n    # Endpoint code\n')))),(0,a.yg)("h2",{id:"deploying-with-docker"},"Deploying with Docker"),(0,a.yg)("p",null,"For easy deployment with Docker:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Create a Dockerfile:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-dockerfile"},'FROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD ["python", "server.py"]\n')),(0,a.yg)("ol",{start:2},(0,a.yg)("li",{parentName:"ol"},"Build and run the Docker container:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"docker build -t gpt-researcher-mcp .\ndocker run -p 8000:8000 -e OPENAI_API_KEY=your_key -e TAVILY_API_KEY=your_key gpt-researcher-mcp\n")),(0,a.yg)("h2",{id:"monitoring-and-logging"},"Monitoring and Logging"),(0,a.yg)("p",null,"Enable detailed logging to monitor server activity:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\',\n    handlers=[\n        logging.FileHandler("mcp_server.log"),\n        logging.StreamHandler()\n    ]\n)\n\nlogger = logging.getLogger("mcp_server")\n')),(0,a.yg)("h2",{id:"extending-functionality"},"Extending Functionality"),(0,a.yg)("p",null,"You can extend the MCP server with additional capabilities:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Add new research tools"),(0,a.yg)("li",{parentName:"ol"},"Implement custom report formats"),(0,a.yg)("li",{parentName:"ol"},"Integrate with additional data sources"),(0,a.yg)("li",{parentName:"ol"},"Add specialized research agents")),(0,a.yg)("p",null,"For example, to add a new tool:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'@app.tool("analyze_sentiment")\nasync def analyze_sentiment(query: str):\n    """Analyze the sentiment of research results."""\n    # Implementation\n    return {"sentiment": "positive", "confidence": 0.87}\n')),(0,a.yg)("h2",{id:"troubleshooting-advanced-issues"},"Troubleshooting Advanced Issues"),(0,a.yg)("h3",{id:"handling-rate-limits"},"Handling Rate Limits"),(0,a.yg)("p",null,"If you encounter rate limits with external APIs:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},"import time\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n\n@retry(wait=wait_exponential(multiplier=1, min=4, max=10), stop=stop_after_attempt(5))\ndef search_with_retry(query):\n    try:\n        return search_engine.search(query)\n    except RateLimitError:\n        time.sleep(5)\n        raise\n")),(0,a.yg)("h3",{id:"memory-management"},"Memory Management"),(0,a.yg)("p",null,"For handling large research tasks:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import gc\n\ndef clean_memory():\n    """Force garbage collection to free memory"""\n    gc.collect()\n')),(0,a.yg)("h2",{id:"next-steps"},"Next Steps"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Explore ",(0,a.yg)("a",{parentName:"li",href:"../frontend/introduction"},"integrating with your own applications")),(0,a.yg)("li",{parentName:"ul"},"Learn about ",(0,a.yg)("a",{parentName:"li",href:"../multi_agents/langgraph"},"creating custom agents")," to enhance research capabilities"),(0,a.yg)("li",{parentName:"ul"},"Contribute to the ",(0,a.yg)("a",{parentName:"li",href:"../../contribute"},"GPT Researcher project"))),(0,a.yg)("p",null,":-)"))}p.isMDXComponent=!0},5680:(e,n,r)=>{r.d(n,{xA:()=>g,yg:()=>m});var t=r(6540);function a(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function i(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),r.push.apply(r,t)}return r}function o(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?i(Object(r),!0).forEach((function(n){a(e,n,r[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))}))}return e}function l(e,n){if(null==e)return{};var r,t,a=function(e,n){if(null==e)return{};var r,t,a={},i=Object.keys(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||(a[r]=e[r]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var s=t.createContext({}),c=function(e){var n=t.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):o(o({},n),e)),r},g=function(e){var n=c(e.components);return t.createElement(s.Provider,{value:n},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},u=t.forwardRef((function(e,n){var r=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,g=l(e,["components","mdxType","originalType","parentName"]),p=c(r),u=a,m=p["".concat(s,".").concat(u)]||p[u]||d[u]||i;return r?t.createElement(m,o(o({ref:n},g),{},{components:r})):t.createElement(m,o({ref:n},g))}));function m(e,n){var r=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=r.length,o=new Array(i);o[0]=u;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[p]="string"==typeof e?e:a,o[1]=l;for(var c=2;c<i;c++)o[c]=r[c];return t.createElement.apply(null,o)}return t.createElement.apply(null,r)}u.displayName="MDXCreateElement"}}]);