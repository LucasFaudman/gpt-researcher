"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2976],{5680:(e,r,t)=>{t.d(r,{xA:()=>l,yg:()=>m});var n=t(6540);function o(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function a(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);r&&(n=n.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,n)}return t}function c(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?a(Object(t),!0).forEach((function(r){o(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function s(e,r){if(null==e)return{};var t,n,o=function(e,r){if(null==e)return{};var t,n,o={},a=Object.keys(e);for(n=0;n<a.length;n++)t=a[n],r.indexOf(t)>=0||(o[t]=e[t]);return o}(e,r);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)t=a[n],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var i=n.createContext({}),p=function(e){var r=n.useContext(i),t=r;return e&&(t="function"==typeof e?e(r):c(c({},r),e)),t},l=function(e){var r=p(e.components);return n.createElement(i.Provider,{value:r},e.children)},u="mdxType",h={inlineCode:"code",wrapper:function(e){var r=e.children;return n.createElement(n.Fragment,{},r)}},d=n.forwardRef((function(e,r){var t=e.components,o=e.mdxType,a=e.originalType,i=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),u=p(t),d=o,m=u["".concat(i,".").concat(d)]||u[d]||h[d]||a;return t?n.createElement(m,c(c({ref:r},l),{},{components:t})):n.createElement(m,c({ref:r},l))}));function m(e,r){var t=arguments,o=r&&r.mdxType;if("string"==typeof e||o){var a=t.length,c=new Array(a);c[0]=d;var s={};for(var i in r)hasOwnProperty.call(r,i)&&(s[i]=r[i]);s.originalType=e,s[u]="string"==typeof e?e:o,c[1]=s;for(var p=2;p<a;p++)c[p]=t[p];return n.createElement.apply(null,c)}return n.createElement.apply(null,t)}d.displayName="MDXCreateElement"},7225:(e,r,t)=>{t.r(r),t.d(r,{contentTitle:()=>c,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>i});var n=t(8168),o=(t(6540),t(5680));const a={},c="Tailored Research",s={unversionedId:"gpt-researcher/context/tailored-research",id:"gpt-researcher/context/tailored-research",isDocsHomePage:!1,title:"Tailored Research",description:"The GPT Researcher package allows you to tailor the research to your needs such as researching on specific sources (URLs) or local documents, and even specify the agent prompt instruction upon which the research is conducted.",source:"@site/docs/gpt-researcher/context/tailored-research.md",sourceDirName:"gpt-researcher/context",slug:"/gpt-researcher/context/tailored-research",permalink:"/docs/gpt-researcher/context/tailored-research",editUrl:"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/context/tailored-research.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Visualizing Websockets",permalink:"/docs/gpt-researcher/frontend/visualizing-websockets"},next:{title:"Local Documents",permalink:"/docs/gpt-researcher/context/local-docs"}},i=[{value:"Research on Specific Sources \ud83d\udcda",id:"research-on-specific-sources-",children:[],level:3},{value:"Specify Agent Prompt \ud83d\udcdd",id:"specify-agent-prompt-",children:[],level:3},{value:"Research on Local Documents \ud83d\udcc4",id:"research-on-local-documents-",children:[],level:3},{value:"Hybrid Research \ud83d\udd04",id:"hybrid-research-",children:[],level:3},{value:"Research on LangChain Documents \ud83e\udd9c\ufe0f\ud83d\udd17",id:"research-on-langchain-documents-\ufe0f",children:[],level:3}],p={toc:i},l="wrapper";function u(e){let{components:r,...t}=e;return(0,o.yg)(l,(0,n.A)({},p,t,{components:r,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"tailored-research"},"Tailored Research"),(0,o.yg)("p",null,"The GPT Researcher package allows you to tailor the research to your needs such as researching on specific sources (URLs) or local documents, and even specify the agent prompt instruction upon which the research is conducted."),(0,o.yg)("h3",{id:"research-on-specific-sources-"},"Research on Specific Sources \ud83d\udcda"),(0,o.yg)("p",null,"You can specify the sources you want the GPT Researcher to research on by providing a list of URLs. The GPT Researcher will then conduct research on the provided sources via ",(0,o.yg)("inlineCode",{parentName:"p"},"source_urls"),". "),(0,o.yg)("p",null,"If you want GPT Researcher to perform additional research outside of the URLs you provided, i.e., conduct research on various other websites that it finds suitable for the query/sub-query, you can set the parameter ",(0,o.yg)("inlineCode",{parentName:"p"},"complement_source_urls")," as ",(0,o.yg)("inlineCode",{parentName:"p"},"True"),". Default value of ",(0,o.yg)("inlineCode",{parentName:"p"},"False")," will only scour the websites you provide via ",(0,o.yg)("inlineCode",{parentName:"p"},"source_urls"),"."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'from gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_report(query: str, report_type: str, sources: list) -> str:\n    researcher = GPTResearcher(query=query, report_type=report_type, source_urls=sources, complement_source_urls=False)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n\nif __name__ == "__main__":\n    query = "What are the biggest trends in AI lately?"\n    report_source = "static"\n    sources = [\n        "https://en.wikipedia.org/wiki/Artificial_intelligence",\n        "https://www.ibm.com/think/insights/artificial-intelligence-trends",\n        "https://www.forbes.com/advisor/business/ai-statistics"\n    ]\n    report = asyncio.run(get_report(query=query, report_source=report_source, sources=sources))\n    print(report)\n')),(0,o.yg)("h3",{id:"specify-agent-prompt-"},"Specify Agent Prompt \ud83d\udcdd"),(0,o.yg)("p",null,"You can specify the agent prompt instruction upon which the research is conducted. This allows you to guide the research in a specific direction and tailor the report layout.\nSimply pass the prompt as the ",(0,o.yg)("inlineCode",{parentName:"p"},"query")," argument to the ",(0,o.yg)("inlineCode",{parentName:"p"},"GPTResearcher"),' class and the "custom_report" ',(0,o.yg)("inlineCode",{parentName:"p"},"report_type"),"."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'from gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_report(prompt: str, report_type: str) -> str:\n    researcher = GPTResearcher(query=prompt, report_type=report_type)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n    \nif __name__ == "__main__":\n    report_type = "custom_report"\n    prompt = "Research the latest advancements in AI and provide a detailed report in APA format including sources."\n\n    report = asyncio.run(get_report(prompt=prompt, report_type=report_type))\n    print(report)\n')),(0,o.yg)("h3",{id:"research-on-local-documents-"},"Research on Local Documents \ud83d\udcc4"),(0,o.yg)("p",null,"You can instruct the GPT Researcher to research on local documents by providing the path to those documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents."),(0,o.yg)("p",null,(0,o.yg)("em",{parentName:"p"},"Step 1"),": Add the env variable ",(0,o.yg)("inlineCode",{parentName:"p"},"DOC_PATH")," pointing to the folder where your documents are located."),(0,o.yg)("p",null,"For example:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-bash"},'export DOC_PATH="./my-docs"\n')),(0,o.yg)("p",null,(0,o.yg)("em",{parentName:"p"},"Step 2"),": When you create an instance of the ",(0,o.yg)("inlineCode",{parentName:"p"},"GPTResearcher")," class, pass the ",(0,o.yg)("inlineCode",{parentName:"p"},"report_source")," argument as ",(0,o.yg)("inlineCode",{parentName:"p"},'"local"'),"."),(0,o.yg)("p",null,"GPT Researcher will then conduct research on the provided documents."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'from gpt_researcher import GPTResearcher\nimport asyncio\n\nasync def get_report(query: str, report_source: str) -> str:\n    researcher = GPTResearcher(query=query, report_source=report_source)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n    \nif __name__ == "__main__":\n    query = "What can you tell me about myself based on my documents?"\n    report_source = "local" # "local" or "web"\n\n    report = asyncio.run(get_report(query=query, report_source=report_source))\n    print(report)\n')),(0,o.yg)("h3",{id:"hybrid-research-"},"Hybrid Research \ud83d\udd04"),(0,o.yg)("p",null,"You can combine the above methods to conduct hybrid research. For example, you can instruct the GPT Researcher to research on both web sources and local documents.\nSimply provide the sources and set the ",(0,o.yg)("inlineCode",{parentName:"p"},"report_source")," argument as ",(0,o.yg)("inlineCode",{parentName:"p"},'"hybrid"')," and watch the magic happen."),(0,o.yg)("p",null,"Please note! You should set the proper retrievers for the web sources and doc path for local documents for this to work.\nTo learn more about retrievers check out the ",(0,o.yg)("a",{parentName:"p",href:"https://docs.gptr.dev/docs/gpt-researcher/search-engines/retrievers"},"Retrievers")," documentation."),(0,o.yg)("h3",{id:"research-on-langchain-documents-\ufe0f"},"Research on LangChain Documents \ud83e\udd9c\ufe0f\ud83d\udd17"),(0,o.yg)("p",null,"You can instruct the GPT Researcher to research on a list of langchain document instances."),(0,o.yg)("p",null,"For example:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'from langchain_core.documents import Document\nfrom typing import List, Dict\nfrom gpt_researcher import GPTResearcher\nfrom langchain_postgres.vectorstores import PGVector\nfrom langchain_openai import OpenAIEmbeddings\nfrom sqlalchemy import create_engine\nimport asyncio\n\n\n\nCONNECTION_STRING = \'postgresql://someuser:somepass@localhost:5432/somedatabase\'\n\ndef get_retriever(collection_name: str, search_kwargs: Dict[str, str]):\n    engine = create_engine(CONNECTION_STRING)\n    embeddings =  OpenAIEmbeddings()\n\n    index = PGVector.from_existing_index(\n        use_jsonb=True,\n        embedding=embeddings,\n        collection_name=collection_name,\n        connection=engine,\n    )\n\n    return index.as_retriever(search_kwargs=search_kwargs)\n\n\nasync def get_report(query: str, report_type: str, report_source: str, documents: List[Document]) -> str:\n    researcher = GPTResearcher(query=query, report_type=report_type, report_source=report_source, documents=documents)\n    await researcher.conduct_research()\n    report = await researcher.write_report()\n    return report\n\nif __name__ == "__main__":\n    query = "What can you tell me about blue cheese based on my documents?"\n    report_type = "research_report"\n    report_source = "langchain_documents"\n\n    # using a LangChain retriever to get all the documents regarding cheese\n    # https://api.python.langchain.com/en/latest/retrievers/langchain_core.retrievers.BaseRetriever.html#langchain_core.retrievers.BaseRetriever.invoke\n    langchain_retriever = get_retriever("cheese_collection", { "k": 3 })\n    documents = langchain_retriever.invoke("All the documents about cheese")\n    report = asyncio.run(get_report(query=query, report_type=report_type, report_source=report_source, documents=documents))\n    print(report)\n')))}u.isMDXComponent=!0}}]);