"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7713],{7393:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"gpt-researcher/mcp-server/getting-started","title":"Getting Started","description":"The GPT Researcher MCP Server provides Model Context Protocol (MCP) integration for GPT Researcher, allowing AI assistants to perform autonomous, comprehensive web research and generate reports via the MCP protocol.","source":"@site/docs/gpt-researcher/mcp-server/getting-started.md","sourceDirName":"gpt-researcher/mcp-server","slug":"/gpt-researcher/mcp-server/getting-started","permalink":"/docs/gpt-researcher/mcp-server/getting-started","draft":false,"unlisted":false,"editUrl":"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/mcp-server/getting-started.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"LangGraph","permalink":"/docs/gpt-researcher/multi_agents/langgraph"},"next":{"title":"Advanced Usage","permalink":"/docs/gpt-researcher/mcp-server/advanced-usage"}}');var t=r(4848),i=r(8453);const a={sidebar_position:1},o="Getting Started",l={},c=[{value:"Why GPT Researcher MCP?",id:"why-gpt-researcher-mcp",level:2},{value:"Resources",id:"resources",level:3},{value:"Primary Tools",id:"primary-tools",level:3},{value:"Prompts",id:"prompts",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Installation",id:"installation",level:2},{value:"Running the MCP Server",id:"running-the-mcp-server",level:2},{value:"Method 1: Directly using Python",id:"method-1-directly-using-python",level:3},{value:"Method 2: Using the MCP CLI (if installed)",id:"method-2-using-the-mcp-cli-if-installed",level:3},{value:"Integrating with Claude",id:"integrating-with-claude",level:2},{value:"Example Usage with Claude",id:"example-usage-with-claude",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Next Steps",id:"next-steps",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"getting-started",children:"Getting Started"})}),"\n",(0,t.jsx)(n.p,{children:"The GPT Researcher MCP Server provides Model Context Protocol (MCP) integration for GPT Researcher, allowing AI assistants to perform autonomous, comprehensive web research and generate reports via the MCP protocol."}),"\n",(0,t.jsx)(n.h2,{id:"why-gpt-researcher-mcp",children:"Why GPT Researcher MCP?"}),"\n",(0,t.jsx)(n.p,{children:"While many AI apps can access web search tools with MCP, GPT Researcher MCP delivers in-depth results. Standard search tools return raw results requiring manual filtering, often containing irrelevant sources and wasting context window space."}),"\n",(0,t.jsx)(n.p,{children:"GPT Researcher performs autonomous, deep research - not just search. It intelligently explores and validates multiple sources, focusing only on relevant and up-to-date information. Though slightly slower (30-40 seconds) than standard search, it delivers higher quality information, optimized context, comprehensive results, and better reasoning for LLMs."}),"\n",(0,t.jsx)(n.p,{children:"The MCP server exposes the following capabilities to AI assistants:"}),"\n",(0,t.jsx)(n.h3,{id:"resources",children:"Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"research_resource"}),": Get web resources related to a given task via research."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"primary-tools",children:"Primary Tools"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"deep_research"}),": Performs autonomous web research on a topic, finding the most reliable and relevant information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"quick_search"}),": Performs a fast web search optimized for speed over quality, returning search results with snippets"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"write_report"}),": Generate a report based on research results"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"get_research_sources"}),": Get the sources used in the research"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"get_research_context"}),": Get the full context of the research"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"prompts",children:"Prompts"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"research_query"}),": Create a research query prompt"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before running the MCP server, make sure you have:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Python 3.10 or higher installed"}),"\n",(0,t.jsxs)(n.li,{children:["API keys for the services you plan to use:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"OpenAI API key"}),"\n",(0,t.jsx)(n.li,{children:"Tavily API key (or other search APIs you plan to use)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Clone the GPT Researcher repository:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/assafelovic/gpt-researcher.git\ncd gpt-researcher\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Install the mcp-server dependencies:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd mcp-server\npip install -r requirements.txt\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsxs)(n.li,{children:["Set up your environment variables:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Copy the ",(0,t.jsx)(n.code,{children:".env.example"})," file to create a new file named ",(0,t.jsx)(n.code,{children:".env"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cp .env.example .env\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Edit the ",(0,t.jsx)(n.code,{children:".env"})," file and add your API keys and configure other settings:"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"OPENAI_API_KEY=your_openai_api_key\nTAVILY_API_KEY=your_tavily_api_key\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"running-the-mcp-server",children:"Running the MCP Server"}),"\n",(0,t.jsx)(n.p,{children:"You can start the MCP server in two ways:"}),"\n",(0,t.jsx)(n.h3,{id:"method-1-directly-using-python",children:"Method 1: Directly using Python"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python server.py\n"})}),"\n",(0,t.jsx)(n.h3,{id:"method-2-using-the-mcp-cli-if-installed",children:"Method 2: Using the MCP CLI (if installed)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mcp run server.py\n"})}),"\n",(0,t.jsx)(n.p,{children:"Once the server is running, you'll see output indicating that the server is ready to accept connections."}),"\n",(0,t.jsx)(n.h2,{id:"integrating-with-claude",children:"Integrating with Claude"}),"\n",(0,t.jsx)(n.p,{children:"There are two primary ways to integrate your MCP server with Claude:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./claude-integration",children:"API Integration"})})," - For programmatic access through Claude API"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./claude-desktop-integration",children:"Desktop Integration"})})," - For using with Claude desktop application on Mac"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For detailed instructions on each method, follow the links above."}),"\n",(0,t.jsx)(n.h2,{id:"example-usage-with-claude",children:"Example Usage with Claude"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'User: I\'m considering investing in NVIDIA. Can you research the current state of the company, recent stock performance, latest product announcements, and analyst opinions?\n\nClaude: I\'ll help you research NVIDIA for investment purposes. This will take a moment as I conduct comprehensive research to find the most up-to-date information about their stock performance, recent developments, and expert analysis.\n\n[Claude uses conduct_research tool - this takes 30-40 seconds]\n\nClaude: Based on my research, here\'s a comprehensive analysis of NVIDIA\'s current state:\n\n## NVIDIA (NVDA) Current Status\n\n### Recent Stock Performance\n- Current price: $942.89 (as of my research)\n- YTD performance: +90.4% \n- 52-week range: $460.30 - $974.00\n- Market cap: ~$2.32 trillion\n\n### Recent Key Developments\n1. **Blackwell AI Platform**: At GTC 2024, NVIDIA unveiled the Blackwell architecture, their next-generation AI chip platform promising 4x performance improvement over Hopper. Major cloud providers (AWS, Google, Microsoft) have already committed to integrating Blackwell.\n\n2. **Q1 FY2025 Earnings (May 2024)**: \n   - Revenue: $26.0 billion (+262% YoY)\n   - Data Center revenue: $22.6 billion (+427% YoY)\n   - Gross margin: 78.4%\n   - Announced 10-for-1 stock split effective June 7, 2024\n\n3. **Supply Chain Expansion**: Increased investments in manufacturing capacity with TSMC to address ongoing chip shortages for their high-demand AI GPUs.\n\n### Analyst Consensus\n- Strong Buy consensus with 37 analysts rating it "Buy," 5 "Hold," and 0 "Sell"\n- Average price target: $1,042 (post-split adjusted: $104.20)\n- Key bull thesis: Continued AI infrastructure spending and NVIDIA\'s dominant position in AI chips\n- Key bear concerns: Valuation, potential competition from AMD, Intel, and custom silicon from tech giants\n\n### Industry Position\n- Dominant in AI GPU market with estimated 80-90% market share\n- Expanding ecosystem through CUDA software platform and industry partnerships\n- Diversifying beyond chips with DGX Cloud, AI software platforms, and automotive solutions\n'})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.p,{children:"If you encounter issues while running the MCP server:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Make sure your API keys are correctly set in the ",(0,t.jsx)(n.code,{children:".env"})," file"]}),"\n",(0,t.jsx)(n.li,{children:"Check that you're using Python 3.10 or higher"}),"\n",(0,t.jsx)(n.li,{children:"Ensure all dependencies are installed correctly"}),"\n",(0,t.jsx)(n.li,{children:"Check the server logs for error messages"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Explore the ",(0,t.jsx)(n.a,{href:"https://docs.anthropic.com/claude/docs/model-context-protocol",children:"MCP protocol documentation"})," to better understand how to integrate with Claude"]}),"\n",(0,t.jsxs)(n.li,{children:["Learn about ",(0,t.jsx)(n.a,{href:"../getting-started/introduction",children:"GPT Researcher's core features"})," to enhance your research capabilities"]}),"\n",(0,t.jsxs)(n.li,{children:["Check out the ",(0,t.jsx)(n.a,{href:"./advanced-usage",children:"Advanced Usage"})," guide for more configuration options"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:":-)"})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var s=r(6540);const t={},i=s.createContext(t);function a(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);