"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7713],{5210:(e,r,t)=>{t.r(r),t.d(r,{contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var n=t(8168),a=(t(6540),t(5680));const i={sidebar_position:1},o="Getting Started",l={unversionedId:"gpt-researcher/mcp-server/getting-started",id:"gpt-researcher/mcp-server/getting-started",isDocsHomePage:!1,title:"Getting Started",description:"The GPT Researcher MCP Server provides Model Context Protocol (MCP) integration for GPT Researcher, allowing AI assistants to perform autonomous, comprehensive web research and generate reports via the MCP protocol.",source:"@site/docs/gpt-researcher/mcp-server/getting-started.md",sourceDirName:"gpt-researcher/mcp-server",slug:"/gpt-researcher/mcp-server/getting-started",permalink:"/docs/gpt-researcher/mcp-server/getting-started",editUrl:"https://github.com/assafelovic/gpt-researcher/tree/master/docs/docs/gpt-researcher/mcp-server/getting-started.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"docsSidebar",previous:{title:"LangGraph",permalink:"/docs/gpt-researcher/multi_agents/langgraph"},next:{title:"Advanced Usage",permalink:"/docs/gpt-researcher/mcp-server/advanced-usage"}},s=[{value:"Why GPT Researcher MCP?",id:"why-gpt-researcher-mcp",children:[{value:"Resources",id:"resources",children:[],level:3},{value:"Primary Tools",id:"primary-tools",children:[],level:3},{value:"Prompts",id:"prompts",children:[],level:3}],level:2},{value:"Prerequisites",id:"prerequisites",children:[],level:2},{value:"Installation",id:"installation",children:[],level:2},{value:"Running the MCP Server",id:"running-the-mcp-server",children:[{value:"Method 1: Directly using Python",id:"method-1-directly-using-python",children:[],level:3},{value:"Method 2: Using the MCP CLI (if installed)",id:"method-2-using-the-mcp-cli-if-installed",children:[],level:3}],level:2},{value:"Integrating with Claude",id:"integrating-with-claude",children:[],level:2},{value:"Example Usage with Claude",id:"example-usage-with-claude",children:[],level:2},{value:"Troubleshooting",id:"troubleshooting",children:[],level:2},{value:"Next Steps",id:"next-steps",children:[],level:2}],c={toc:s},p="wrapper";function u(e){let{components:r,...t}=e;return(0,a.yg)(p,(0,n.A)({},c,t,{components:r,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"getting-started"},"Getting Started"),(0,a.yg)("p",null,"The GPT Researcher MCP Server provides Model Context Protocol (MCP) integration for GPT Researcher, allowing AI assistants to perform autonomous, comprehensive web research and generate reports via the MCP protocol."),(0,a.yg)("h2",{id:"why-gpt-researcher-mcp"},"Why GPT Researcher MCP?"),(0,a.yg)("p",null,"While many AI apps can access web search tools with MCP, GPT Researcher MCP delivers superior results. Standard search tools return raw results requiring manual filtering, often containing irrelevant sources and wasting context window space."),(0,a.yg)("p",null,"GPT Researcher performs autonomous, comprehensive research - not just search. It intelligently explores and validates multiple sources, focusing only on relevant and up-to-date information. Though slightly slower (30-40 seconds) than standard search, it delivers higher quality information, optimized context, comprehensive results, and better reasoning for LLMs."),(0,a.yg)("p",null,"The MCP server exposes the following capabilities to AI assistants:"),(0,a.yg)("h3",{id:"resources"},"Resources"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"research_resource"),": Get web resources related to a given task via research.")),(0,a.yg)("h3",{id:"primary-tools"},"Primary Tools"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"conduct_research"),": Performs autonomous web research on a topic, finding the most reliable and relevant information"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"write_report"),": Generate a report based on research results"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"get_research_sources"),": Get the sources used in the research"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"get_research_context"),": Get the full context of the research")),(0,a.yg)("h3",{id:"prompts"},"Prompts"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"research_query"),": Create a research query prompt")),(0,a.yg)("h2",{id:"prerequisites"},"Prerequisites"),(0,a.yg)("p",null,"Before running the MCP server, make sure you have:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Python 3.10 or higher installed"),(0,a.yg)("li",{parentName:"ol"},"API keys for the services you plan to use:",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"OpenAI API key"),(0,a.yg)("li",{parentName:"ul"},"Tavily API key (or other search APIs you plan to use)")))),(0,a.yg)("h2",{id:"installation"},"Installation"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Clone the GPT Researcher repository:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/assafelovic/gpt-researcher.git\ncd gpt-researcher\n")),(0,a.yg)("ol",{start:2},(0,a.yg)("li",{parentName:"ol"},"Install the mcp-server dependencies:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"cd mcp-server\npip install -r requirements.txt\n")),(0,a.yg)("ol",{start:3},(0,a.yg)("li",{parentName:"ol"},"Set up your environment variables:",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"Copy the ",(0,a.yg)("inlineCode",{parentName:"li"},".env.example")," file to create a new file named ",(0,a.yg)("inlineCode",{parentName:"li"},".env"),":")),(0,a.yg)("pre",{parentName:"li"},(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"cp .env.example .env\n")),(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"Edit the ",(0,a.yg)("inlineCode",{parentName:"li"},".env")," file and add your API keys and configure other settings:")),(0,a.yg)("pre",{parentName:"li"},(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"OPENAI_API_KEY=your_openai_api_key\nTAVILY_API_KEY=your_tavily_api_key\n")))),(0,a.yg)("h2",{id:"running-the-mcp-server"},"Running the MCP Server"),(0,a.yg)("p",null,"You can start the MCP server in two ways:"),(0,a.yg)("h3",{id:"method-1-directly-using-python"},"Method 1: Directly using Python"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"python server.py\n")),(0,a.yg)("h3",{id:"method-2-using-the-mcp-cli-if-installed"},"Method 2: Using the MCP CLI (if installed)"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"mcp run server.py\n")),(0,a.yg)("p",null,"Once the server is running, you'll see output indicating that the server is ready to accept connections."),(0,a.yg)("h2",{id:"integrating-with-claude"},"Integrating with Claude"),(0,a.yg)("p",null,"There are two primary ways to integrate your MCP server with Claude:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},(0,a.yg)("a",{parentName:"strong",href:"./claude-integration"},"API Integration"))," - For programmatic access through Claude API"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},(0,a.yg)("a",{parentName:"strong",href:"./claude-desktop-integration"},"Desktop Integration"))," - For using with Claude desktop application on Mac")),(0,a.yg)("p",null,"For detailed instructions on each method, follow the links above."),(0,a.yg)("h2",{id:"example-usage-with-claude"},"Example Usage with Claude"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},'User: I\'m considering investing in NVIDIA. Can you research the current state of the company, recent stock performance, latest product announcements, and analyst opinions?\n\nClaude: I\'ll help you research NVIDIA for investment purposes. This will take a moment as I conduct comprehensive research to find the most up-to-date information about their stock performance, recent developments, and expert analysis.\n\n[Claude uses conduct_research tool - this takes 30-40 seconds]\n\nClaude: Based on my research, here\'s a comprehensive analysis of NVIDIA\'s current state:\n\n## NVIDIA (NVDA) Current Status\n\n### Recent Stock Performance\n- Current price: $942.89 (as of my research)\n- YTD performance: +90.4% \n- 52-week range: $460.30 - $974.00\n- Market cap: ~$2.32 trillion\n\n### Recent Key Developments\n1. **Blackwell AI Platform**: At GTC 2024, NVIDIA unveiled the Blackwell architecture, their next-generation AI chip platform promising 4x performance improvement over Hopper. Major cloud providers (AWS, Google, Microsoft) have already committed to integrating Blackwell.\n\n2. **Q1 FY2025 Earnings (May 2024)**: \n   - Revenue: $26.0 billion (+262% YoY)\n   - Data Center revenue: $22.6 billion (+427% YoY)\n   - Gross margin: 78.4%\n   - Announced 10-for-1 stock split effective June 7, 2024\n\n3. **Supply Chain Expansion**: Increased investments in manufacturing capacity with TSMC to address ongoing chip shortages for their high-demand AI GPUs.\n\n### Analyst Consensus\n- Strong Buy consensus with 37 analysts rating it "Buy," 5 "Hold," and 0 "Sell"\n- Average price target: $1,042 (post-split adjusted: $104.20)\n- Key bull thesis: Continued AI infrastructure spending and NVIDIA\'s dominant position in AI chips\n- Key bear concerns: Valuation, potential competition from AMD, Intel, and custom silicon from tech giants\n\n### Industry Position\n- Dominant in AI GPU market with estimated 80-90% market share\n- Expanding ecosystem through CUDA software platform and industry partnerships\n- Diversifying beyond chips with DGX Cloud, AI software platforms, and automotive solutions\n')),(0,a.yg)("h2",{id:"troubleshooting"},"Troubleshooting"),(0,a.yg)("p",null,"If you encounter issues while running the MCP server:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Make sure your API keys are correctly set in the ",(0,a.yg)("inlineCode",{parentName:"li"},".env")," file"),(0,a.yg)("li",{parentName:"ol"},"Check that you're using Python 3.10 or higher"),(0,a.yg)("li",{parentName:"ol"},"Ensure all dependencies are installed correctly"),(0,a.yg)("li",{parentName:"ol"},"Check the server logs for error messages")),(0,a.yg)("h2",{id:"next-steps"},"Next Steps"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Explore the ",(0,a.yg)("a",{parentName:"li",href:"https://docs.anthropic.com/claude/docs/model-context-protocol"},"MCP protocol documentation")," to better understand how to integrate with Claude"),(0,a.yg)("li",{parentName:"ul"},"Learn about ",(0,a.yg)("a",{parentName:"li",href:"../getting-started/introduction"},"GPT Researcher's core features")," to enhance your research capabilities"),(0,a.yg)("li",{parentName:"ul"},"Check out the ",(0,a.yg)("a",{parentName:"li",href:"./advanced-usage"},"Advanced Usage")," guide for more configuration options")),(0,a.yg)("p",null,":-)"))}u.isMDXComponent=!0},5680:(e,r,t)=>{t.d(r,{xA:()=>p,yg:()=>h});var n=t(6540);function a(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function i(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);r&&(n=n.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?i(Object(t),!0).forEach((function(r){a(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function l(e,r){if(null==e)return{};var t,n,a=function(e,r){if(null==e)return{};var t,n,a={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],r.indexOf(t)>=0||(a[t]=e[t]);return a}(e,r);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=n.createContext({}),c=function(e){var r=n.useContext(s),t=r;return e&&(t="function"==typeof e?e(r):o(o({},r),e)),t},p=function(e){var r=c(e.components);return n.createElement(s.Provider,{value:r},e.children)},u="mdxType",g={inlineCode:"code",wrapper:function(e){var r=e.children;return n.createElement(n.Fragment,{},r)}},d=n.forwardRef((function(e,r){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),d=a,h=u["".concat(s,".").concat(d)]||u[d]||g[d]||i;return t?n.createElement(h,o(o({ref:r},p),{},{components:t})):n.createElement(h,o({ref:r},p))}));function h(e,r){var t=arguments,a=r&&r.mdxType;if("string"==typeof e||a){var i=t.length,o=new Array(i);o[0]=d;var l={};for(var s in r)hasOwnProperty.call(r,s)&&(l[s]=r[s]);l.originalType=e,l[u]="string"==typeof e?e:a,o[1]=l;for(var c=2;c<i;c++)o[c]=t[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,t)}d.displayName="MDXCreateElement"}}]);